## testing code here so it wont lost 
## creating future month 

from pyspark.sql import functions as F

# Read the table
df = spark.read.table("fpnacopilot.data_engineering.transform_data")

# Find max year and month from month_id
max_month_id_row = df.agg(F.max("month_id").alias("max_month_id")).collect()[0]
max_month_id = max_month_id_row["max_month_id"]
max_year, max_month = map(int, max_month_id.split("-"))


# Generate all months from the next month after max_month_id to the end of that year (dynamic)
missing_months = []
year, month = max_year, max_month + 1
while month <= 12:
    missing_months.append((year, month, f"{year}-{month:02d}"))
    month += 1

if missing_months:
    schema = ["year", "month", "month_id"]
    new_df = spark.createDataFrame(missing_months, schema)
    # Add missing columns with nulls to match the table schema
    for col in df.columns:
        if col not in new_df.columns:
            new_df = new_df.withColumn(col, F.lit(None).cast(df.schema[col].dataType))
    # Reorder columns to match
    new_df = new_df.select(df.columns)
    # Union and display
    result_df = df.unionByName(new_df)
else:
    result_df = df

display(result_df)


#########################

##parameterization

model_config = spark.read.table("fpnacopilot.data_engineering.model_config").toPandas()

#model_path  = model_config['model']
trained_year = model_config['X_train']
test_year = model_config['X_test']

parts = full_path.split("/")
inference_year = None
inference_month = None
for i, part in enumerate(parts):
    if part.isdigit() and len(part) == 4:
        inference_year = int(part)
        if i + 1 < len(parts):
            month_part = parts[i + 1]
            # Convert month name to 2-digit number if needed
            month_map = {
                'jan': '01', 'feb': '02', 'mar': '03', 'apr': '04', 'may': '05', 'jun': '06',
                'jul': '07', 'aug': '08', 'sep': '09', 'oct': '10', 'nov': '11', 'dec': '12'
            }
            month_lower = month_part.lower()[:3]
            inference_month = month_map.get(month_lower, month_part)
        break

# Create month_id variable as "YYYY-MM"
month_id = f"{inference_year}-{inference_month}"
print(inference_month , inference_year )
print(trained_year , test_year)
print(month_id)



###########################

##Reciprocal transformer
from sklearn.base import BaseEstimator, TransformerMixin
from dataclasses import dataclass, field
from typing import List, Optional
import numpy as np

class ReciprocalTransformer(BaseEstimator, TransformerMixin):
    """
    Apply reciprocal transformation (1/x) to specified columns.
    Handles zeros and near-zeros by adding a small epsilon.
    """
    def __init__(self, epsilon: float = 1e-8):
        self.epsilon = epsilon
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        X = np.array(X, dtype=np.float64)
        # Add epsilon to avoid division by zero
        X_safe = np.where(np.abs(X) < self.epsilon, np.sign(X) * self.epsilon + self.epsilon, X)
        return 1.0 / X_safe
    
    def get_feature_names_out(self, input_features=None):
        if input_features is None:
            return None
        return [f"recip_{f}" for f in input_features]



@dataclass
class ImputationConfig:
    """Config for imputation logic used during inference on future months."""
    month_id_col: str = "month_id"           
    actual_col: str = "actual_filled"        
    year_col: str = "year"
    month_col: str = "month"                 


@dataclass
class RegressionPipelineConfig:
    target: str
    date_col: str                       # column containing date/datetime or year-like value
    train_years: List[int] = field(default_factory=lambda: [2023, 2024])
    test_year: int = 2025

    normalize: bool = True              # scaling
    transformation: bool = False        # optional power transform
    reciprocal_cols: List[str] = field(default_factory=list)  # columns to apply 1/x transform

    feature_selection: bool = True
    feature_selection_estimator: str = "lightgbm"
    n_features_to_select: float = 0.7   # fraction or int

  
    fold: int = 5
    session_id: int = 42

    # Model training
    optimize_metric: str = "RMSE"       
    top_n: int = 5                     

    # SGU identifier for model naming
    sgu: str = "all"                    

    # Inference / imputation (for future-month predictions)
    imputation_config: Optional[ImputationConfig] = None


########################################################

##loading trained model
import joblib
model_path = f"/Volumes/ml_model/local_model_upload_test/exl_rfc_model_cutoff_{inference_year - 1}.joblib"
artifact = joblib.load(model_path)
display(artifact["model"])

############################################

##impuatation and inference


import pandas as pd
import numpy as np
import random
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any, Tuple, Union
import warnings
import os

# Suppress noisy warnings
warnings.filterwarnings("ignore", category=UserWarning, module="openpyxl")
warnings.filterwarnings("ignore", category=UserWarning, module="joblib")
warnings.filterwarnings("ignore", category=UserWarning, module="sklearn")
warnings.filterwarnings("ignore", message=".*ConvergenceWarning.*")
os.environ["LOKY_MAX_CPU_COUNT"] = "4"  # Suppress loky CPU core detection warning

from sklearn.base import BaseEstimator, TransformerMixin


class ReciprocalTransformer(BaseEstimator, TransformerMixin):
    """
    Apply reciprocal transformation (1/x) to specified columns.
    Handles zeros and near-zeros by adding a small epsilon.
    """
    def __init__(self, epsilon: float = 1e-8):
        self.epsilon = epsilon
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        X = np.array(X, dtype=np.float64)
        # Add epsilon to avoid division by zero
        X_safe = np.where(np.abs(X) < self.epsilon, np.sign(X) * self.epsilon + self.epsilon, X)
        return 1.0 / X_safe
    
    def get_feature_names_out(self, input_features=None):
        if input_features is None:
            return None
        return [f"recip_{f}" for f in input_features]

# Serialization & prediction intervals
import joblib
from scipy import stats as scipy_stats


# ============================================================================
# PREDICTION INTERVAL MODULE (Z-Score / T-Score based)
# ============================================================================

@dataclass
class PredictionIntervalResult:
    """Container for prediction interval results."""
    point_estimate: float
    confidence_levels: List[float]
    lower_bounds: Dict[float, float]
    upper_bounds: Dict[float, float]
    std_error: float
    n_samples: int
    n_features: int
    degrees_of_freedom: int
    distribution_used: str  # "z" or "t"


def compute_prediction_intervals(
    prediction: float,
    residual_std: float,
    n_samples: int,
    n_features: int,
    confidence_levels: List[float] = [0.95, 0.85, 0.75, 0.65],
) -> PredictionIntervalResult:
    """
    Compute prediction intervals using z-score or t-score based on sample size.
    
    Parameters:
    -----------
    prediction : float
        Point estimate (model prediction)
    residual_std : float
        Standard deviation of residuals (RMSE or std of errors)
    n_samples : int
        Number of training samples
    n_features : int
        Number of features in the model
    confidence_levels : List[float]
        List of confidence levels (e.g., [0.95, 0.85, 0.75, 0.65])
    
    Returns:
    --------
    PredictionIntervalResult with bounds for each confidence level
    
    Notes:
    ------
    - Uses t-distribution if n_samples < 30 (small sample)
    - Uses z-distribution (normal) if n_samples >= 30 (large sample)
    - Degrees of freedom = n_samples - n_features - 1
    """
    # Calculate degrees of freedom
    df = max(1, n_samples - n_features - 1)
    
    # Decide distribution: t for small samples, z for large
    use_t_distribution = n_samples < 30
    distribution_used = "t" if use_t_distribution else "z"
    
    # Standard error of prediction (simplified - using residual std as proxy)
    # For a more accurate SE, you'd need: SE = σ * sqrt(1 + 1/n + leverage)
    std_error = residual_std
    
    lower_bounds = {}
    upper_bounds = {}
    
    for conf_level in confidence_levels:
        alpha = 1 - conf_level
        
        if use_t_distribution:
            # t-distribution critical value
            critical_value = scipy_stats.t.ppf(1 - alpha / 2, df=df)
        else:
            # z-distribution critical value
            critical_value = scipy_stats.norm.ppf(1 - alpha / 2)
        
        margin = critical_value * std_error
        lower_bounds[conf_level] = prediction - margin
        upper_bounds[conf_level] = prediction + margin
    
    return PredictionIntervalResult(
        point_estimate=prediction,
        confidence_levels=confidence_levels,
        lower_bounds=lower_bounds,
        upper_bounds=upper_bounds,
        std_error=std_error,
        n_samples=n_samples,
        n_features=n_features,
        degrees_of_freedom=df,
        distribution_used=distribution_used,
    )


def predict_with_intervals_from_joblib(
    joblib_path: str,
    X_input: pd.DataFrame,
    confidence_levels: List[float] = [0.95, 0.85, 0.75, 0.65],
    residual_std: Optional[float] = None,
) -> pd.DataFrame:
    """
    Load a saved model from joblib and predict with confidence intervals.
    
    Parameters:
    -----------
    joblib_path : str
        Path to the saved .joblib file
    X_input : pd.DataFrame
        Input features for prediction (raw, before preprocessing)
    confidence_levels : List[float]
        Confidence levels for intervals (default: [0.95, 0.85, 0.75, 0.65])
    residual_std : float, optional
        Residual standard deviation. If None, will try to compute from artifact.
    
    Returns:
    --------
    pd.DataFrame with columns:
        - prediction: point estimate
        - std_error: standard error
        - distribution: 'z' or 't'
        - lower_XX%, upper_XX% for each confidence level
    
    Example:
    --------
    >>> df = predict_with_intervals_from_joblib(
    ...     "model/exl_rfc_model_cutoff_2025.joblib",
    ...     X_new,
    ...     confidence_levels=[0.95, 0.85, 0.75, 0.65]
    ... )
    """
    # Load artifact
    artifact = joblib.load(joblib_path)
    model = artifact["model"]
    preprocessor = artifact["preprocessor"]
    feature_selector = artifact.get("feature_selector")
    config = artifact.get("config")
    raw_feature_columns = artifact.get("raw_feature_columns", [])
    stored_residual_std = artifact.get("residual_std")
    stored_n_samples = artifact.get("n_training_samples")
    stored_n_features = artifact.get("n_features")
    
    # Get training info from artifact or config
    if stored_n_samples is not None:
        n_samples = stored_n_samples
    elif config is not None:
        train_years = config.train_years
        # Calculate n_samples: each year has 12 months
        n_samples = len(train_years) * 12
    else:
        # Default assumption if config not available
        n_samples = 36  # 3 years * 12 months
    
    # Get number of features
    if stored_n_features is not None:
        n_features = stored_n_features
    elif raw_feature_columns:
        n_features = len(raw_feature_columns)
    else:
        n_features = X_input.shape[1]
    
    # Preprocess input
    if raw_feature_columns:
        X_raw = X_input[raw_feature_columns].copy() if all(c in X_input.columns for c in raw_feature_columns) else X_input.copy()
    else:
        X_raw = X_input.copy()
    
    X_processed = preprocessor.transform(X_raw)
    if feature_selector is not None:
        X_processed = feature_selector.transform(X_processed)
    
    # Make predictions
    predictions = model.predict(X_processed)
    
    # Use residual std from: 1) argument, 2) stored in artifact, 3) estimate
    if residual_std is None:
        if stored_residual_std is not None:
            residual_std = stored_residual_std
            print(f"  Using stored residual_std: {residual_std:,.2f}")
        elif hasattr(model, 'cv_results_') and 'mean_test_score' in model.cv_results_:
            # Approximate from CV score
            residual_std = abs(model.cv_results_['mean_test_score'].mean()) ** 0.5
        else:
            # Use 5% of mean prediction as rough estimate (fallback)
            residual_std = np.std(predictions) * 0.1 if len(predictions) > 1 else abs(predictions[0]) * 0.05
            print(f"  Warning: residual_std not available, using estimate: {residual_std:,.2f}")
    
    # Build results
    results = []
    for i, pred in enumerate(predictions):
        interval_result = compute_prediction_intervals(
            prediction=pred,
            residual_std=residual_std,
            n_samples=n_samples,
            n_features=n_features,
            confidence_levels=confidence_levels,
        )
        
        row = {
            "prediction": pred,
            "std_error": interval_result.std_error,
            "n_samples": interval_result.n_samples,
            "df": interval_result.degrees_of_freedom,
            "distribution": interval_result.distribution_used,
        }
        
        for conf_level in confidence_levels:
            pct = int(conf_level * 100)
            row[f"lower_{pct}%"] = interval_result.lower_bounds[conf_level]
            row[f"upper_{pct}%"] = interval_result.upper_bounds[conf_level]
        
        results.append(row)
    
    return pd.DataFrame(results)


def print_prediction_intervals(
    prediction: float,
    residual_std: float,
    n_samples: int,
    n_features: int,
    confidence_levels: List[float] = [0.95, 0.85, 0.75, 0.65],
    label: str = "",
) -> PredictionIntervalResult:
    """
    Compute and print prediction intervals in a formatted table.
    
    Parameters:
    -----------
    prediction : float
        Point estimate
    residual_std : float
        Residual standard deviation (RMSE)
    n_samples : int
        Number of training samples
    n_features : int
        Number of features
    confidence_levels : List[float]
        Confidence levels to compute
    label : str
        Optional label for the prediction (e.g., month name)
    
    Returns:
    --------
    PredictionIntervalResult object
    """
    result = compute_prediction_intervals(
        prediction=prediction,
        residual_std=residual_std,
        n_samples=n_samples,
        n_features=n_features,
        confidence_levels=confidence_levels,
    )
    
    print(f"\n{'='*70}")
    if label:
        print(f"PREDICTION INTERVALS: {label}")
    else:
        print("PREDICTION INTERVALS")
    print(f"{'='*70}")
    print(f"Point Estimate:     {result.point_estimate:>20,.2f}")
    print(f"Standard Error:     {result.std_error:>20,.2f}")
    print(f"Training Samples:   {result.n_samples:>20}")
    print(f"Features:           {result.n_features:>20}")
    print(f"Degrees of Freedom: {result.degrees_of_freedom:>20}")
    print(f"Distribution:       {result.distribution_used.upper():>20} ({'t-score' if result.distribution_used == 't' else 'z-score'})")
    print(f"{'-'*70}")
    print(f"{'Confidence':<15}{'Lower Bound':>20}{'Upper Bound':>20}{'Range Width':>15}")
    print(f"{'-'*70}")
    
    for conf_level in sorted(result.confidence_levels, reverse=True):
        pct = int(conf_level * 100)
        lower = result.lower_bounds[conf_level]
        upper = result.upper_bounds[conf_level]
        width = upper - lower
        print(f"{pct}%{'':<12}{lower:>20,.2f}{upper:>20,.2f}{width:>15,.2f}")
    
    print(f"{'='*70}")
    
    return result


# Step 0.5 — Imputation config and helpers (for future-month inference)

@dataclass
class ImputationConfig:
    """Config for imputation logic used during inference on future months."""
    month_id_col: str = "month_id"           # column with YYYY-MM format
    actual_col: str = "actual_filled"        # column with actual revenue (or "revenue")
    year_col: str = "year"
    month_col: str = "month"                 # numeric month 1-12


# --- Imputation helper functions (from Imputation_FutureMonths.ipynb) ---

def compute_hist_revision_ratio(df_hist: pd.DataFrame, month_id_col: str = "month_id") -> Dict[int, float]:
    """Compute month-wise historical patterns of how remaining forecasts are revised."""
    df_hist = df_hist.sort_values(month_id_col).reset_index(drop=True)
    rows = []
    for i in range(len(df_hist) - 1):
        r0, r1 = df_hist.iloc[i], df_hist.iloc[i + 1]
        if r0["year"] != r1["year"]:
            continue
        if any(pd.isna([r0["forecast_roy"], r1["forecast_roy"], r0["actual_filled"]])):
            continue
        if r0["forecast_roy"] <= 0:
            continue
        revision = (r1["forecast_roy"] - (r0["forecast_roy"] - r0["actual_filled"])) / r0["forecast_roy"]
        rows.append({"month": r0["month"], "revision": revision})
    if not rows:
        return {}
    rev_df = pd.DataFrame(rows)
    return rev_df.groupby("month")["revision"].apply(
        lambda s: s.ewm(span=6, adjust=False).mean().iloc[-1]
    ).to_dict()


def compute_hist_category_shares(df_hist: pd.DataFrame) -> Dict[str, float]:
    """Compute historical proportions of remaining forecast across revenue categories."""
    total = df_hist["forecast_roy"].sum()
    if total <= 0:
        return {"signed": 1/3, "unsigned": 1/3, "pipeline": 1/3}
    return {
        "signed": df_hist["fcst_committed_signed_rem"].sum() / total,
        "unsigned": df_hist["fcst_committed_unsigned_rem"].sum() / total,
        "pipeline": df_hist["fcst_wtd_pipeline_rem"].sum() / total,
    }


def compute_hist_prob_anchor(df_hist: pd.DataFrame) -> Dict[int, float]:
    """Establish historical, month-specific baseline for mean_prob_pct_wtd_pip."""
    return df_hist.groupby("month")["mean_prob_pct_wtd_pip"].median().to_dict()


def update_forecast_state(
    df: pd.DataFrame,
    target_month: str,
    mnum: int,
    state: Dict[str, float],
    actual: float,
    revision_ratio: float,
    shares: Dict[str, float],
    month_id_col: str = "month_id",
) -> Dict[str, float]:
    """Update forecast state from one month to next (burn-down + revision)."""
    shares = dict(shares)
    shares["signed"] += shares["signed"] * random.uniform(0.015, 0.030)
    shares["unsigned"] -= shares["unsigned"] * random.uniform(0.08, 0.12)
    shares["pipeline"] = 1 - (shares["signed"] + shares["unsigned"])
    state["forecast_roy"] -= actual
    revision_amt = revision_ratio * state["forecast_roy"]
    state["forecast_roy"] += revision_amt
    row = df.loc[df[month_id_col] == target_month]
    if len(row) > 0 and "actual_mean_3" in df.columns:
        last_3_mean = row["actual_mean_3"].iloc[0]
        if pd.notna(last_3_mean) and last_3_mean > 0:
            tot_rem = last_3_mean * 1.025 * (12 - mnum)
            state["forecast_roy"] = min(state["forecast_roy"], tot_rem)
    state["fcst_signed_rem"] = state["forecast_roy"] * shares["signed"]
    state["fcst_unsigned_rem"] = state["forecast_roy"] * shares["unsigned"]
    state["fcst_pipeline_rem"] = state["forecast_roy"] * shares["pipeline"]
    for k in state:
        state[k] = max(0.0, state[k])
    return state
    

def rolling_slope(values, window):
    x = np.arange(window)
    coeffs = np.polyfit(x, values[-window:], 1)
    return coeffs[0]


def recompute_ly_features(df: pd.DataFrame, target_month: str, month_id_col: str = "month_id") -> pd.DataFrame:
    """
    Recompute Last Year (LY) features for a target month by looking up corresponding month from previous year.
    This is critical for future-month predictions where LY data exists but isn't pre-computed.
    """
    df = df.copy()
    # Parse target month to get last year's corresponding month
    target_year = int(target_month[:4])
    target_month_num = int(target_month[5:7])
    ly_year = target_year - 1
    ly_month_id = f"{ly_year}-{target_month_num:02d}"
    
    # Get last year's data for the corresponding month
    ly_mask = df[month_id_col] == ly_month_id
    target_mask = df[month_id_col] == target_month
    
    if ly_mask.any():
        ly_row = df.loc[ly_mask].iloc[0]
        
        # LY1CM: Last Year's Current Month actual revenue
        ly1cm = ly_row.get("actual_filled", ly_row.get("revenue", np.nan))
        if pd.notna(ly1cm):
            df.loc[target_mask, "LY1CM"] = ly1cm
        
        # LY1_YTD: Last Year's YTD at the same point
        # Sum all actual_filled from Jan to target_month_num of last year
        ly_ytd_mask = (df["year"] == ly_year) & (df["month"] <= target_month_num)
        if "actual_filled" in df.columns:
            ly1_ytd = df.loc[ly_ytd_mask, "actual_filled"].sum()
        else:
            ly1_ytd = df.loc[ly_ytd_mask, "revenue"].sum() if "revenue" in df.columns else np.nan
        if pd.notna(ly1_ytd) and ly1_ytd > 0:
            df.loc[target_mask, "LY1_YTD"] = ly1_ytd
        
        # LY1_CM_trend_slope_3: Last Year's 3-month trend slope ending at current month
        # Get last 3 months of LY data ending at ly_month_id
        ly_3m_end = target_month_num
        ly_3m_start = max(1, target_month_num - 2)
        ly_3m_mask = (df["year"] == ly_year) & (df["month"] >= ly_3m_start) & (df["month"] <= ly_3m_end)
        ly_3m_values = df.loc[ly_3m_mask, "actual_filled" if "actual_filled" in df.columns else "revenue"].dropna()
        if len(ly_3m_values) >= 2:
            # Simple slope: (last - first) / (n - 1)
            slope = (ly_3m_values.iloc[-1] - ly_3m_values.iloc[0]) / max(1, len(ly_3m_values) - 1)
            df.loc[target_mask, "LY1_CM_trend_slope_3"] = slope
        
        # LY1_ROLL3M_SLOPE: Same as above (rolling 3-month slope)
        if len(ly_3m_values) >= 2:
            df.loc[target_mask, "LY1_ROLL3M_SLOPE"] = slope
    
    return df


def recompute_actual_features(df: pd.DataFrame, actual_ytd_state: float, month_id_col: str = "month_id") -> pd.DataFrame:
    """Recompute features from actual_sim (actual + predicted) during recursive forecasting."""
    df = df.copy().sort_values(month_id_col)
    df["lag1_actual"] = df["actual_sim"].shift(1)
    df["lag3_actual"] = df["actual_sim"].shift(3)
    df["LY1CM"] = df["actual_sim"].shift(12)
    df["actual_mean_3"] = df["actual_sim"].shift(1).rolling(3, min_periods=1).mean()
    df["actual_mean_6"] = df["actual_sim"].shift(1).rolling(6, min_periods=1).mean()
    df["ROLL3M_STD"] = df["actual_sim"].shift(1).rolling(3, min_periods=1).std()
    df["Avg_LM1_LM2_LM3"] = df["actual_sim"].shift(1).rolling(3, min_periods=1).mean()
    df["LY1_ROLL3M_SLOPE"] = (
        df["actual_sim"]
          .shift(13)
          .rolling(3, min_periods=3)
          .apply(lambda s: rolling_slope(s.values, len(s)), raw=False)
    )
    df["LY1_CM_trend_slope_3"] = (
        df["actual_sim"]
          .shift(12)
          .rolling(3, min_periods=3)
          .apply(lambda s: rolling_slope(s.values, len(s)), raw=False)
    )
    g = df.shift(12).groupby("year")["actual_filled"]
    df["LY1_YTD"] = g.cumsum() - df["actual_filled"].shift(12)
    df["LM1_LM3_MOM"] = df["lag1_actual"] - df["lag3_actual"]
    if "CM_forecast" in df.columns and "lag1_actual" in df.columns:
        df["prev_month_achv_pct"] = df["CM_forecast"].shift(1) * 100 / df["lag1_actual"].replace(0, np.nan)
    # months_left computed at runtime as 13 - month (no separate column in data)
    if "month" in df.columns and "forecast_roy" in df.columns:
        months_left = (13 - df["month"]).replace(0, np.nan)
        df["total_per_month_left"] = df["forecast_roy"] / months_left
    df["CY_LY_ROLL3M_MEAN_diff"] = df["actual_sim"].shift(1).rolling(3).mean() - df["actual_sim"].shift(13).rolling(3).mean()
    if "month" in df.columns and "fcst_committed_signed_rem" in df.columns:
        months_left = (13 - df["month"]).replace(0, np.nan)
        df["Committed - Signed_monthly_AVG"] = df["fcst_committed_signed_rem"] / months_left
    if "fcst_wtd_pipeline_rem" in df.columns:
        df["Wtd. Pipeline_ROLL3M_AVG"] = df["fcst_wtd_pipeline_rem"].pct_change(fill_method=None).rolling(3, min_periods=1).mean()
    if "fcst_committed_signed_rem" in df.columns:
        df["Committed - Signed_ROLL3M_AVG"] = df["fcst_committed_signed_rem"].pct_change(fill_method=None).rolling(3, min_periods=1).mean()
    df["YTD"] = actual_ytd_state
    return df


def recompute_forecast_features(
    df: pd.DataFrame, state: Dict[str, float], actual_ytd_state: float
) -> pd.DataFrame:
    """Recompute forecast-derived features from state."""
    df = df.copy()
    fcst_yearly = state["forecast_roy"] + actual_ytd_state
    boost = random.uniform(1.001, 1.0075)
    prev_fcst = df["Forecast_yearly"].iloc[0] if "Forecast_yearly" in df.columns else fcst_yearly
    df["Forecast_yearly"] = (prev_fcst * boost) if fcst_yearly < prev_fcst else fcst_yearly
    df["Forecast_YTD_diff"] = state["forecast_roy"] - actual_ytd_state
    tot = state["forecast_roy"]
    df["Committed_Signed_revenue_ratio"] = state["fcst_signed_rem"] / tot if tot > 0 else 0
    df["Committed_Unsigned_revenue_ratio"] = state["fcst_unsigned_rem"] / tot if tot > 0 else 0
    s_plus_u = state["fcst_signed_rem"] + state["fcst_unsigned_rem"]
    df["Signed - TotalCommitted_revenue_ratio"] = state["fcst_signed_rem"] / s_plus_u if s_plus_u > 0 else 0
    return df


def compute_current_year_deviation(
    df: pd.DataFrame, vintage_month: str, hist_prob_anchor: Dict[int, float], month_id_col: str = "month_id"
) -> float:
    """Measure how current year's pipeline confidence at vintage deviates from historical."""
    row = df.loc[df[month_id_col] == vintage_month].iloc[0]
    mnum = int(row["month"])
    hist_level = hist_prob_anchor.get(mnum)
    curr_level = row["mean_prob_pct_wtd_pip"]
    if hist_level is None or pd.isna(curr_level):
        return 0.0
    return float(curr_level - hist_level)


def propagate_deviation(base_deviation: float, step: int, decay: float = 1.0) -> float:
    """Control how current-year deviation decays over future months."""
    return base_deviation * (decay ** step)


def recompute_mean_prob_pct_wtd_pip(
    target_month_num: int,
    months_ahead: int,
    hist_prob_anchor: Dict[int, float],
    base_deviation: float,
    decay: float = 1.025,
    lower: float = 1,
    upper: float = 100,
) -> float:
    """Combine historical seasonality with current-year deviation for future pipeline prob."""
    hist_level = hist_prob_anchor.get(target_month_num, np.mean(list(hist_prob_anchor.values())) if hist_prob_anchor else 50)
    adj = propagate_deviation(base_deviation, months_ahead, decay)
    return float(np.clip(hist_level + adj, lower, upper))


# Step 1 — Config dataclass

@dataclass
class RegressionPipelineConfig:
    target: str
    date_col: str                       # column containing date/datetime or year-like value
    train_years: List[int] = field(default_factory=lambda: [2023, 2024])
    test_year: int = 2025

    # Feature engineering (scaling) + basic preprocessing
    normalize: bool = True              # scaling
    transformation: bool = False        # optional power transform
    reciprocal_cols: List[str] = field(default_factory=list)  # columns to apply 1/x transform

    # Feature selection
    feature_selection: bool = True
    feature_selection_estimator: str = "lightgbm"
    n_features_to_select: float = 0.7   # fraction or int

    # CV / reproducibility
    fold: int = 5
    session_id: int = 42

    # Model training
    optimize_metric: str = "RMSE"       # RMSE / MAE / R2 / MAPE
    top_n: int = 5                      # top N models to return

    # SGU identifier for model naming
    sgu: str = "all"                    # "all" or specific SGU name

    # Inference / imputation (for future-month predictions)
    imputation_config: Optional[ImputationConfig] = None


# Inference-only pipeline (load + predict_from_saved_model)

class SklearnRegressionPipeline:
    """Inference-only: load saved artifact and predict_from_saved_model (imputation)."""

    @staticmethod
    def load(filepath: str) -> Dict[str, Any]:
        """Load a saved model artifact."""
        return joblib.load(filepath)

    @staticmethod
    def predict_from_saved_model(
        filepath_or_artifact: Union[str, Dict[str, Any]],
        df_base: pd.DataFrame,
        vintage_month: Union[str, List[str]],
        print_features: bool = True,
        include_features: bool = True,
        include_intervals: bool = True,
        training_start_year: int = 2023,
        confidence_levels: List[float] = [0.95, 0.85, 0.75, 0.65],
    ) -> pd.DataFrame:
        """
        Run inference on future months using imputation.
        filepath_or_artifact: path to .joblib file (str) or already-loaded artifact (dict from joblib.load).
        df_base: pandas DataFrame with month_id, actual_filled (or revenue), forecast columns.
        vintage_month: single vintage (e.g. "2025-03") or list of vintages for batch.
        print_features: if True, print feature values for each month prediction.
        include_features: if True, include imputed feature values in output DataFrame.
        include_intervals: if True, include prediction intervals at confidence_levels.
        training_start_year: first year of training data (default 2023).
        confidence_levels: confidence levels for prediction intervals (default [0.95, 0.85, 0.75, 0.65]).
        Returns: pandas DataFrame (no Spark).
        """
        vintages = [vintage_month] if isinstance(vintage_month, str) else vintage_month
        if not vintages:
            raise ValueError("vintage_month must be a string or non-empty list.")
        if isinstance(filepath_or_artifact, dict):
            artifact = filepath_or_artifact
        else:
            artifact = joblib.load(filepath_or_artifact)
        model = artifact["model"]
        preprocessor = artifact["preprocessor"]
        feature_selector = artifact.get("feature_selector")
        raw_feature_columns = artifact.get("raw_feature_columns")
        if not raw_feature_columns and "preprocessor" in artifact:
            preproc = artifact["preprocessor"]
            raw_feature_columns = []
            for _name, _trans, cols in preproc.transformers_:
                raw_feature_columns.extend(cols)
        if not raw_feature_columns:
            raise ValueError("Artifact has no raw_feature_columns. Retrain and save to include them.")
        hist_revision_ratio = artifact.get("hist_revision_ratio")
        hist_prob_anchor = artifact.get("hist_prob_anchor")
        imputation_config = artifact.get("imputation_config") or ImputationConfig()
        month_id_col = imputation_config.month_id_col
        actual_col = imputation_config.actual_col
        df_base = df_base.copy()
        if actual_col != "actual_filled":
            df_base["actual_filled"] = df_base.get(actual_col, df_base.get("revenue"))
        if "month" not in df_base.columns and month_id_col in df_base.columns:
            df_base["month"] = pd.to_datetime(df_base[month_id_col], format="%Y-%m", errors="coerce").dt.month
        # If artifact has no imputation params, compute from provided historical data
        if hist_revision_ratio is None or hist_prob_anchor is None:
            print("  Artifact has no imputation params; computing from historical_df.")
            if hist_revision_ratio is None:
                hist_revision_ratio = compute_hist_revision_ratio(df_base, month_id_col)
            if hist_prob_anchor is None:
                hist_prob_anchor = compute_hist_prob_anchor(df_base)
        
        # Get residual_std for prediction intervals
        residual_std = artifact.get("residual_std")
        n_features = len(raw_feature_columns)
        
        if include_intervals and residual_std is None:
            print("  Warning: residual_std not in saved artifact. Intervals will use estimate.")
        imp_cfg = imputation_config
        if "month" not in df_base.columns and month_id_col in df_base.columns:
            df_base["month"] = pd.to_datetime(df_base[month_id_col], format="%Y-%m", errors="coerce").dt.month
        all_preds = []
        for vintage_month_single in vintages:
            df = df_base.copy()
            df["actual_sim"] = np.nan
            df.loc[df[month_id_col] < vintage_month_single, "actual_sim"] = df.loc[df[month_id_col] < vintage_month_single, "actual_filled"]
            row = df.loc[df[month_id_col] == vintage_month_single].iloc[0]
            state = {
                "forecast_roy": row["forecast_roy"],
                "fcst_signed_rem": row["fcst_committed_signed_rem"],
                "fcst_unsigned_rem": row["fcst_committed_unsigned_rem"],
                "fcst_pipeline_rem": row["fcst_wtd_pipeline_rem"],
            }
            base_deviation = compute_current_year_deviation(df, vintage_month_single, hist_prob_anchor, month_id_col)
            months = df[(df["year"] == row["year"]) & (df[month_id_col] >= vintage_month_single)].sort_values(month_id_col)[month_id_col].tolist()
            actual_ytd_state = df.loc[(df[month_id_col] < vintage_month_single) & (df["year"] == int(vintage_month_single[:4])), "actual_filled"].sum()
            hist_category_shares = compute_hist_category_shares(df[df[month_id_col] == vintage_month_single])
            preds = []
            
            # Calculate n_samples for prediction intervals
            # Training data: from training_start_year to (vintage_year - 1), 12 rows per year
            vintage_year = int(vintage_month_single[:4])
            n_training_years = vintage_year - training_start_year  # e.g., 2026 - 2023 = 3 years (2023, 2024, 2025)
            n_samples = n_training_years * 12
            
            # Estimate residual_std if not available
            effective_residual_std = residual_std
            if effective_residual_std is None:
                # Fallback: estimate as ~5% of typical prediction magnitude
                effective_residual_std = 10_000_000  # placeholder, will be updated after first prediction
            
            if print_features:
                print(f"\n{'='*80}")
                print(f"RECURSIVE FORECAST FROM VINTAGE: {vintage_month_single}")
                print(f"Training samples: {n_samples} ({n_training_years} years × 12 months)")
                print(f"Distribution: {'t-score' if n_samples < 30 else 'z-score'} (df={max(1, n_samples - n_features - 1)})")
                print(f"{'='*80}")
            
            for i, target_month in enumerate(months):
                # Write state into target month row so recompute functions can compute total_per_month_left
                mask = df[month_id_col] == target_month
                if "forecast_roy" in df.columns:
                    df.loc[mask, "forecast_roy"] = state["forecast_roy"]
                if "fcst_committed_signed_rem" in df.columns:
                    df.loc[mask, "fcst_committed_signed_rem"] = state["fcst_signed_rem"]
                if "fcst_committed_unsigned_rem" in df.columns:
                    df.loc[mask, "fcst_committed_unsigned_rem"] = state["fcst_unsigned_rem"]
                if "fcst_wtd_pipeline_rem" in df.columns:
                    df.loc[mask, "fcst_wtd_pipeline_rem"] = state["fcst_pipeline_rem"]
                
                # Recompute LY (Last Year) features first - critical for future months
                df = recompute_ly_features(df, target_month, month_id_col)
                df = recompute_actual_features(df, actual_ytd_state, month_id_col)
                df = recompute_forecast_features(df, state, actual_ytd_state)
                mnum = int(target_month[5:7])
                df.loc[df[month_id_col] == target_month, "mean_prob_pct_wtd_pip"] = recompute_mean_prob_pct_wtd_pip(
                    mnum, i, hist_prob_anchor, base_deviation
                )
                
                # Compute prev_month_achv_pct using Forecast_Jan..Forecast_Dec columns
                _MONTH_NAMES = ["Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"]
                prev_month_num = (mnum - 1) if mnum > 1 else 12
                forecast_col = "Forecast_" + _MONTH_NAMES[prev_month_num - 1]
                if forecast_col in df.columns and "lag1_actual" in df.columns:
                    if mnum == 1:
                        dec_row = df.loc[df[month_id_col] == f"{int(target_month[:4]) - 1}-12"]
                        forecast_val = dec_row[forecast_col].iloc[0] if len(dec_row) > 0 and forecast_col in dec_row.columns else np.nan
                    else:
                        forecast_val = row.get(forecast_col) if forecast_col in row.index else np.nan
                    lag1 = df.loc[mask, "lag1_actual"].iloc[0]
                    if pd.notna(forecast_val) and lag1 is not None and lag1 != 0 and np.isfinite(lag1):
                        df.loc[mask, "prev_month_achv_pct"] = float(forecast_val) * 100 / float(lag1)
                    else:
                        # Fallback: use previous month's achievement if available
                        if i > 0:
                            prev_mask = df[month_id_col] == months[i-1]
                            prev_achv = df.loc[prev_mask, "prev_month_achv_pct"].iloc[0] if prev_mask.any() else np.nan
                            df.loc[mask, "prev_month_achv_pct"] = prev_achv if pd.notna(prev_achv) else 95.0  # default fallback
                        else:
                            df.loc[mask, "prev_month_achv_pct"] = 95.0  # reasonable default
                
                # Get raw features - smart NaN handling for slope columns
                X_raw = df.loc[df[month_id_col] == target_month, raw_feature_columns].copy()
                for col in raw_feature_columns:
                    if col in X_raw.columns and X_raw[col].isna().any():
                        if 'slope' in col.lower():
                            # For slope columns, use a small non-zero value to avoid reciprocal explosion
                            X_raw[col] = X_raw[col].fillna(1e-6)
                        else:
                            X_raw[col] = X_raw[col].fillna(0.0)
                X_processed = preprocessor.transform(X_raw)
                if feature_selector is not None:
                    X_processed = feature_selector.transform(X_processed)
                y_hat = float(model.predict(X_processed)[0])
                actual_ytd_state += y_hat
                actual_val = df.loc[df[month_id_col] == target_month, "actual_filled"].values[0]
                
                # Calculate MAPE % between actual and prediction
                if actual_val == 0 or pd.isna(actual_val):
                    mape_pct = np.nan
                else:
                    mape_pct = abs((actual_val - y_hat) / actual_val) * 100
                
                # Calculate prediction intervals
                interval_result = None
                if include_intervals:
                    # Update residual_std estimate if not available from artifact
                    if residual_std is None and i == 0:
                        # Estimate as ~5% of first prediction
                        effective_residual_std = abs(y_hat) * 0.05
                    
                    interval_result = compute_prediction_intervals(
                        prediction=y_hat,
                        residual_std=effective_residual_std,
                        n_samples=n_samples,
                        n_features=n_features,
                        confidence_levels=confidence_levels,
                    )
                
                # Build prediction record
                pred_record = {
                    "vintage": vintage_month_single,
                    "target_month": target_month,
                    "predicted_revenue": y_hat,
                    "actual_revenue": actual_val,
                    "mape_pct": mape_pct,
                }
                
                # Add interval columns if requested
                if include_intervals and interval_result:
                    pred_record["distribution"] = interval_result.distribution_used
                    pred_record["std_error"] = interval_result.std_error
                    for conf_level in confidence_levels:
                        pct = int(conf_level * 100)
                        pred_record[f"lower_{pct}%"] = interval_result.lower_bounds[conf_level]
                        pred_record[f"upper_{pct}%"] = interval_result.upper_bounds[conf_level]
                
                # Add feature values to record if requested
                if include_features:
                    for col in raw_feature_columns:
                        pred_record[f"feat_{col}"] = X_raw[col].values[0] if col in X_raw.columns else np.nan
                
                # Print feature values if requested
                if print_features:
                    print(f"\n--- {target_month} ---")
                    if not pd.isna(mape_pct):
                        print(f"  Prediction: {y_hat:,.2f} | Actual: {actual_val:,.2f} | MAPE: {mape_pct:.2f}%")
                    else:
                        print(f"  Prediction: {y_hat:,.2f} | Actual: N/A")
                    
                    # Print prediction intervals
                    if include_intervals and interval_result:
                        print(f"  Prediction Intervals ({interval_result.distribution_used}-score):")
                        for conf_level in confidence_levels:
                            pct = int(conf_level * 100)
                            lower = interval_result.lower_bounds[conf_level]
                            upper = interval_result.upper_bounds[conf_level]
                            print(f"    {pct}% CI: [{lower:,.2f}, {upper:,.2f}]")
                    
                    print(f"  Features used:")
                    for col in raw_feature_columns:
                        val = X_raw[col].values[0] if col in X_raw.columns else np.nan
                        if isinstance(val, (int, float)) and not pd.isna(val):
                            print(f"    {col}: {val:,.4f}")
                        else:
                            print(f"    {col}: {val}")
                
                preds.append(pred_record)
                df.loc[df[month_id_col] == target_month, "actual_sim"] = y_hat
                if i < len(months) - 1:
                    revision_ratio = hist_revision_ratio.get(mnum, 0.0)
                    state = update_forecast_state(df, target_month, mnum, state, y_hat, revision_ratio, hist_category_shares, month_id_col)
            
            if print_features:
                valid_mapes = [p['mape_pct'] for p in preds if not pd.isna(p['mape_pct'])]
                if valid_mapes:
                    print(f"\n{'='*80}")
                    print(f"Mean MAPE % for {vintage_month_single}: {np.mean(valid_mapes):.2f}%")
                    print(f"{'='*80}")
            
            all_preds.append(pd.DataFrame(preds))
        
        # Combine all predictions
        result_df = pd.concat(all_preds, ignore_index=True)
        
        # Create yearly summary with aggregated intervals
        if include_intervals:
            import json
            yearly_summaries = []
            
            for vintage_month_single in vintages:
                vintage_year = int(vintage_month_single[:4])
                vintage_month_num = int(vintage_month_single[5:7])
                
                # Filter predictions for this vintage
                vintage_preds = result_df[result_df["vintage"] == vintage_month_single]
                
                # Get actuals for months BEFORE vintage_month (Jan to vintage_month - 1)
                # These are actual revenues from the historical data
                actuals_ytd = 0.0
                actuals_detail = []
                if vintage_month_num > 1:
                    for m in range(1, vintage_month_num):
                        month_id = f"{vintage_year}-{m:02d}"
                        actual_row = df_base[df_base[month_id_col] == month_id]
                        if len(actual_row) > 0:
                            actual_val = actual_row["actual_filled"].iloc[0] if "actual_filled" in actual_row.columns else actual_row.get("revenue", pd.Series([np.nan])).iloc[0]
                            if pd.notna(actual_val):
                                actuals_ytd += actual_val
                                actuals_detail.append({"month": month_id, "actual_revenue": actual_val, "source": "actual"})
                
                # Sum of predictions for months from vintage_month to Dec
                predictions_sum = vintage_preds["predicted_revenue"].sum()
                
                # Total yearly revenue = actuals (Jan to vintage-1) + predictions (vintage to Dec)
                yearly_total = actuals_ytd + predictions_sum
                
                # For yearly intervals, only apply uncertainty to the PREDICTED portion
                # Actuals have no uncertainty
                if "std_error" in vintage_preds.columns:
                    monthly_std_errors = vintage_preds["std_error"].values
                    yearly_std_error = np.sqrt(np.sum(monthly_std_errors ** 2))
                else:
                    # Fallback if std_error not available
                    yearly_std_error = predictions_sum * 0.05
                
                # Calculate yearly intervals using the same distribution
                n_training_years = vintage_year - training_start_year
                n_samples = n_training_years * 12
                df_val = max(1, n_samples - n_features - 1)
                use_t = n_samples < 30
                
                # Convert to millions (divide by 10^6) and round to 2 decimal places
                yearly_total_millions = round(yearly_total / 1e6, 2)
                actuals_ytd_millions = round(actuals_ytd / 1e6, 2)
                predictions_sum_millions = round(predictions_sum / 1e6, 2)
                yearly_std_error_millions = yearly_std_error / 1e6
                
                yearly_intervals = {}
                for conf_level in confidence_levels:
                    pct = int(conf_level * 100)
                    alpha = 1 - conf_level
                    if use_t:
                        critical_value = scipy_stats.t.ppf(1 - alpha / 2, df=df_val)
                    else:
                        critical_value = scipy_stats.norm.ppf(1 - alpha / 2)
                    margin_millions = round(critical_value * yearly_std_error_millions, 2)
                    yearly_intervals[f"{pct}%_prediction_range"] = f"{yearly_total_millions} ± {margin_millions} M"
                
                # Build feature set JSON - collect features for each month (actuals + predictions)
                feature_set_list = []
                
                # Add actuals for months before vintage
                for actual_info in actuals_detail:
                    feature_set_list.append({
                        "target_month": actual_info["month"],
                        "revenue_millions": round(actual_info["actual_revenue"] / 1e6, 2),
                        "source": "actual"
                    })
                
                # Add predictions for vintage month onwards
                feat_cols = [c for c in vintage_preds.columns if c.startswith("feat_")]
                for _, row in vintage_preds.iterrows():
                    month_features = {
                        "target_month": row["target_month"],
                        "revenue_millions": round(row["predicted_revenue"] / 1e6, 2),
                        "source": "predicted"
                    }
                    for feat_col in feat_cols:
                        feature_name = feat_col.replace("feat_", "")
                        val = row[feat_col]
                        month_features[feature_name] = float(val) if pd.notna(val) else None
                    feature_set_list.append(month_features)
                
                feature_set_json = json.dumps(feature_set_list, indent=2)
                
                # Build summary record (all values in millions)
                summary_record = {
                    "year": vintage_year,
                    "month": vintage_month_num,
                    "actuals_ytd_millions": actuals_ytd_millions,
                    "predictions_sum_millions": predictions_sum_millions,
                    "yearly_total_millions": yearly_total_millions,
                }
                summary_record.update(yearly_intervals)
                summary_record["feature_set"] = feature_set_json
                
                yearly_summaries.append(summary_record)
            
            # Create summary DataFrame
            summary_df = pd.DataFrame(yearly_summaries)
            
            # Print the summary
            print(f"\n{'='*80}")
            print("YEARLY PREDICTION SUMMARY")
            print(f"{'='*80}")
            
            for _, row in summary_df.iterrows():
                print(f"\nVintage: {int(row['year'])}-{int(row['month']):02d}")
                if int(row['month']) > 1:
                    print(f"  Actuals YTD (Jan to Month {int(row['month'])-1}): {row['actuals_ytd_millions']} M")
                else:
                    print(f"  Actuals YTD: 0.0 M (vintage is January)")
                print(f"  Predictions (Month {int(row['month'])} to Dec): {row['predictions_sum_millions']} M")
                print(f"  Yearly Total: {row['yearly_total_millions']} M")
                for conf_level in confidence_levels:
                    pct = int(conf_level * 100)
                    col_name = f"{pct}%_prediction_range"
                    if col_name in row:
                        print(f"  {pct}% Range: {row[col_name]}")
                print(f"\n  Feature Set (JSON):")
                # Print first 500 chars of feature set for readability
                feat_preview = row['feature_set'][:500] + "..." if len(row['feature_set']) > 500 else row['feature_set']
                print(f"  {feat_preview}")
            
            print(f"\n{'='*80}")
            
            # Save/update yearly_summary.csv - replace existing rows or append new ones
            csv_path = 'yearly_summary.csv'
            import os
            if os.path.exists(csv_path):
                # Load existing CSV
                existing_df = pd.read_csv(csv_path)
                
                # For each new summary row, check if year+month combination exists
                for _, new_row in summary_df.iterrows():
                    new_year = new_row['year']
                    new_month = new_row['month']
                    
                    # Find matching row in existing data
                    match_mask = (existing_df['year'] == new_year) & (existing_df['month'] == new_month)
                    
                    if match_mask.any():
                        # Replace existing row
                        existing_df.loc[match_mask, summary_df.columns] = new_row.values
                        print(f"  Updated existing row for {int(new_year)}-{int(new_month):02d}")
                    else:
                        # Append new row
                        existing_df = pd.concat([existing_df, pd.DataFrame([new_row])], ignore_index=True)
                        print(f"  Appended new row for {int(new_year)}-{int(new_month):02d}")
                
                # Save updated DataFrame
                existing_df.to_csv(csv_path, index=False)
                print(f"\n  Saved to {csv_path} ({len(existing_df)} total rows)")
            else:
                # Create new file
                summary_df.to_csv(csv_path, index=False)
                print(f"\n  Created {csv_path} ({len(summary_df)} rows)")
            
            # Store summary_df as attribute for access
            result_df.attrs['yearly_summary'] = summary_df
        
        return result_df


# Alias for backwards compatibility
PyCaretRegressionPipeline = SklearnRegressionPipeline


# =============================================================================
# Inference only (no training). All inputs/outputs are pandas DataFrames (no Spark).
# In a notebook: load model in a previous cell, e.g. artifact = joblib.load(model_path)
# =============================================================================

if __name__ == "__main__":
    # Input: result_df (pandas or Spark). Your table name is result_df.
    result_df = globals().get("result_df")
    if result_df is None:
        raise NameError(
            "result_df not defined. Set result_df to your input data: "
            "a pandas DataFrame, or e.g. result_df = spark.table('catalog.schema.table_name')"
        )
    # Convert to pandas for inference (Spark DataFrame has toPandas())
    if hasattr(result_df, "toPandas"):
        df = result_df.toPandas()
    else:
        df = result_df.copy() if isinstance(result_df, pd.DataFrame) else result_df

    # Verify required columns for inference
    required_cols = [
        "month_id", "year", "month",
        "revenue",
        "forecast_roy", "fcst_committed_signed_rem",
        "fcst_committed_unsigned_rem", "fcst_wtd_pipeline_rem",
        "mean_prob_pct_wtd_pip",
    ]
    missing = [c for c in required_cols if c not in df.columns]
    if missing:
        raise ValueError(f"Missing required columns for inference: {missing}")
    print("All required columns present. Inference only (no training).")

    # Use already-loaded artifact from previous cell (e.g. artifact = joblib.load(model_path))
    artifact = globals().get("artifact")
    if artifact is None:
        raise NameError(
            "artifact not defined. In a previous cell run: "
            "artifact = joblib.load(model_path)  # e.g. model_path = '/Volumes/.../exl_rfc_model_cutoff_2025.joblib'"
        )
    vintage_month = month_id ## passing vintage month

    # Predict using pre-loaded artifact (no joblib.load here)
    imputed_table = SklearnRegressionPipeline.predict_from_saved_model(
        artifact,
        df,
        vintage_month,
        print_features=True,
        include_features=True,
        include_intervals=True,
    )

    # Result is pandas DataFrame only (no Spark tables or views)
    print("\nPrediction results (vintage, target_month, predicted_revenue, actual_revenue, feat_*):")
    print(imputed_table.to_string())


####################################################################
##creating forecast output table
from __future__ import annotations

import json
import re
from typing import Any, Dict, List, Optional, Union

import joblib
import numpy as np
import pandas as pd
from scipy import stats as scipy_stats


# --- Config: override these to match your schema ---
CONFIG = {
    "history": {
        "vintage_year_col": "year",
        "vintage_month_col": "month",
        "month_id_col": "month_id",
        "forecast_col_pattern": r"forecast[_\-]?(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|\d{1,2})\b",
        "forecast_col_flags": re.IGNORECASE,
    },
    "imputed": {
        "vintage_col": "vintage",
        "target_month_col": "target_month",
        "predicted_revenue_col": "predicted_revenue",
        "std_error_col": "std_error",
        "features_list_col": None,
        "feat_prefix": "feat_",
        "meta_cols": ("vintage", "target_month", "predicted_revenue", "actual_revenue", "std_error", "mape_pct", "distribution"),
    },
    "output": {
        "entity_type": "EXL",
        "entity_name": "NA",
        "time_dim": "Yearly",
        "identifier": "95%",
    },
    "fpna_abfs": {
        "revenue_category_col": "Revenue_Category",
        "exclude_categories": ("Risk", "Opportunity"),
        "month_id_col": None,
        "forecast_col_pattern": r"forecast[_\-]?(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|\d{1,2})\b",
        "forecast_col_flags": re.IGNORECASE,
    },
}


def _month_id_to_year_month(month_id: str) -> tuple[int, int]:
    """Parse 'YYYY-MM' to (year, month)."""
    parts = str(month_id).strip().split("-")
    if len(parts) != 2:
        raise ValueError(f"Invalid month_id: {month_id}. Expected YYYY-MM.")
    return int(parts[0]), int(parts[1])


def _get_forecast_columns(df: pd.DataFrame, pattern: str, flags: int) -> list[str]:
    """Return list of column names that match the monthly forecast pattern."""
    out = []
    for col in df.columns:
        if re.search(pattern, str(col), flags=flags):
            out.append(col)
    return out


def _get_history_row(
    history_df: pd.DataFrame,
    vintage_year: int,
    vintage_month: int,
    year_col: str,
    month_col: str,
    month_id_col: str,
    vintage_month_str: str,
) -> pd.Series:
    """Get the single row in history_df that corresponds to the vintage (year, month)."""
    if month_id_col and month_id_col in history_df.columns:
        mask = history_df[month_id_col].astype(str).str.strip() == str(vintage_month_str).strip()
    else:
        mask = (
            (history_df[year_col].astype(int) == vintage_year)
            & (history_df[month_col].astype(int) == vintage_month)
        )
    rows = history_df.loc[mask]
    if rows.empty:
        raise ValueError(
            f"No row in history_df for vintage {vintage_month_str} "
            f"(year={vintage_year}, month={vintage_month})."
        )
    if len(rows) > 1:
        rows = rows.iloc[[0]]
    return rows.iloc[0]


def _fpna_forecast_from_history(
    history_df: pd.DataFrame,
    vintage_month: str,
    forecast_cols: list[str],
    year_col: str,
    month_col: str,
    month_id_col: Optional[str],
) -> float:
    """Sum of forecast_jan..forecast_dec (or matching columns) from history_df for the vintage row."""
    vintage_year, vintage_month_num = _month_id_to_year_month(vintage_month)
    row = _get_history_row(
        history_df, vintage_year, vintage_month_num,
        year_col, month_col, month_id_col, vintage_month,
    )
    total = 0.0
    for col in forecast_cols:
        if col in row.index:
            val = row[col]
            if pd.notna(val):
                total += float(val)
    return total


def fpna_forecast_from_abfs_df(
    df: pd.DataFrame,
    vintage_month: Optional[str] = None,
    month_id_col: Optional[str] = None,
    revenue_category_col: str = "Revenue_Category",
    exclude_categories: tuple[str, ...] = ("Risk", "Opportunity"),
    forecast_col_pattern: Optional[str] = None,
    forecast_col_flags: int = re.IGNORECASE,
) -> float:
    """
    Compute fpna_forecast from an ABFS-style DataFrame (e.g. from spark.read.csv(...).toPandas()).

    - Filter out rows where Revenue_Category is in exclude_categories (default: Risk, Opportunity).
    - Sum all columns from Forecast_Jan through Forecast_Dec (columns matching forecast_col_pattern).
    - If vintage_month and month_id_col are provided, filter df to that month first.

    Returns:
        Sum of forecast_jan..forecast_dec over the filtered rows.
    """
    cfg = CONFIG.get("fpna_abfs", {})
    if forecast_col_pattern is None:
        forecast_col_pattern = cfg.get("forecast_col_pattern", r"forecast[_\-]?(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|\d{1,2})\b")

    out = df.copy()
    if vintage_month is not None and month_id_col and month_id_col in out.columns:
        out = out.loc[out[month_id_col].astype(str).str.strip() == str(vintage_month).strip()]

    if revenue_category_col in out.columns:
        excl = set(exclude_categories)
        out = out.loc[~out[revenue_category_col].astype(str).str.strip().str.lower().isin({s.strip().lower() for s in excl})]

    forecast_cols = _get_forecast_columns(out, forecast_col_pattern, forecast_col_flags)
    if not forecast_cols:
        return 0.0
    total = out[forecast_cols].apply(pd.to_numeric, errors="coerce").fillna(0).sum().sum()
    return float(total)


def _sum_previous_months_actual(
    history_df: pd.DataFrame,
    vintage_month: str,
    month_id_col: str = "month_id",
    actual_filled_col: str = "actual_filled",
) -> float:
    """
    Sum actual_filled from history_df for months in the SAME YEAR as the vintage
    that are strictly before the vintage month (e.g. vintage 2026-03 -> Jan and Feb 2026 only).
    Never looks beyond the same year (no cross-year lookback).
    """
    vy, vm = _month_id_to_year_month(vintage_month)
    if month_id_col not in history_df.columns or actual_filled_col not in history_df.columns:
        return 0.0
    total = 0.0
    for m in range(1, vm):
        month_id = f"{vy}-{m:02d}"
        mask = history_df[month_id_col].astype(str).str.strip() == month_id
        rows = history_df.loc[mask, actual_filled_col]
        total += rows.apply(pd.to_numeric, errors="coerce").fillna(0).sum()
    return float(total)


def _sum_predicted_revenue_vintage_through_dec(
    imputed_df: pd.DataFrame,
    vintage_month: str,
    vintage_col: str = "vintage",
    target_month_col: str = "target_month",
    predicted_revenue_col: str = "predicted_revenue",
) -> float:
    """
    Sum predicted_revenue in imputed_table only for target_month in the SAME YEAR
    from vintage month through December (e.g. vintage 2026-03 -> 2026-03, 2026-04, ..., 2026-12).
    Does not include any month before the vintage or from another year.
    """
    vy, vm = _month_id_to_year_month(vintage_month)
    mask_vintage = imputed_df[vintage_col].astype(str).str.strip() == str(vintage_month).strip()
    subset = imputed_df.loc[mask_vintage]
    if subset.empty or target_month_col not in subset.columns:
        return 0.0
    total = 0.0
    for _, row in subset.iterrows():
        tm = row.get(target_month_col)
        if pd.isna(tm):
            continue
        try:
            ty, tm_num = _month_id_to_year_month(str(tm).strip())
        except Exception:
            continue
        if ty != vy:
            continue
        if tm_num < vm:
            continue
        val = row.get(predicted_revenue_col)
        if pd.notna(val):
            total += float(pd.to_numeric(val, errors="coerce") or 0)
    return total


def compute_absolute_model_forecast(
    imputed_table: pd.DataFrame,
    history_df: pd.DataFrame,
    vintage_month: str,
    vintage_col: str = "vintage",
    target_month_col: str = "target_month",
    predicted_revenue_col: str = "predicted_revenue",
    month_id_col: str = "month_id",
    actual_filled_col: str = "actual_filled",
) -> float:
    """
    absolute_model_forecast (same year only):
    = actual_filled for Jan..(vintage_month-1) in same year (from history_df)
    + predicted_revenue for vintage_month..Dec in same year (from imputed_table).

    Example: vintage 2026-03 -> actual_filled(2026-01, 2026-02) + predicted(2026-03..2026-12).
    """
    pred_sum = _sum_predicted_revenue_vintage_through_dec(
        imputed_table, vintage_month, vintage_col, target_month_col, predicted_revenue_col
    )
    prev_actual_sum = _sum_previous_months_actual(
        history_df, vintage_month, month_id_col, actual_filled_col
    )
    return pred_sum + prev_actual_sum


# Confidence-level identifiers
IDENTIFIERS = ["95%", "85%", "75%", "65%"]

# Map identifier string -> confidence level float
_IDENTIFIER_TO_CONF = {
    "95%": 0.95,
    "85%": 0.85,
    "75%": 0.75,
    "65%": 0.65,
}


def load_model_artifact(artifact_path: str) -> Dict[str, Any]:
    """
    Load model artifact from a joblib file and extract n_samples, n_features, residual_std.

    Expected keys in the artifact dict:
        - model, preprocessor, feature_selector, config, raw_feature_columns
        - residual_std, n_training_samples, n_features

    Args:
        artifact_path: path to the .joblib file (local or DBFS).

    Returns:
        Dict with keys: model, preprocessor, feature_selector, config,
        raw_feature_columns, residual_std, n_training_samples, n_features.
    """
    artifact = joblib.load(artifact_path)
    return {
        "model": artifact["model"],
        "preprocessor": artifact["preprocessor"],
        "feature_selector": artifact.get("feature_selector"),
        "config": artifact.get("config"),
        "raw_feature_columns": artifact.get("raw_feature_columns", []),
        "residual_std": artifact.get("residual_std"),
        "n_training_samples": artifact.get("n_training_samples"),
        "n_features": artifact.get("n_features"),
    }


def _compute_yearly_std_error(
    imputed_df: pd.DataFrame,
    vintage_month: str,
    vintage_col: str = "vintage",
    std_error_col: str = "std_error",
) -> float:
    """
    Compute yearly_std_error as sqrt(sum(monthly_std_errors^2)) for a vintage.

    Only applies to the PREDICTED portion (months from vintage to Dec).
    Actuals have no uncertainty — margin of error only covers predicted months.
    """
    mask = imputed_df[vintage_col].astype(str).str.strip() == str(vintage_month).strip()
    subset = imputed_df.loc[mask]
    if subset.empty or std_error_col not in subset.columns:
        return 0.0
    monthly_std_errors = subset[std_error_col].apply(pd.to_numeric, errors="coerce").fillna(0).values
    return float(np.sqrt(np.sum(monthly_std_errors ** 2)))


def _margin_of_error_for_identifier(
    imputed_df: pd.DataFrame,
    vintage_month: str,
    vintage_col: str,
    identifier: str,
    std_error_col: str = "std_error",
    n_samples: int = 100,
    n_features: int = 10,
) -> float:
    """
    Compute margin_of_error (in millions) for a confidence-level identifier.

    Applied to the absolute_model_forecast (actuals + predictions).
    Uncertainty only covers the PREDICTED portion; actuals have zero error.

    Steps (matching the imputation script logic):
    1. yearly_std_error = sqrt(sum(monthly_std_errors^2)) for predicted months only.
    2. yearly_std_error_millions = yearly_std_error / 1e6.
    3. n_samples and n_features come from the model artifact.
    4. df_val = max(1, n_samples - n_features - 1).
    5. Use t-distribution if n_samples < 30, else z (normal).
    6. critical_value from the chosen distribution at (1 - alpha/2).
    7. margin_millions = round(critical_value * yearly_std_error_millions, 2).

    Args:
        imputed_df: imputed_table DataFrame.
        vintage_month: vintage month string (YYYY-MM).
        vintage_col: column name for vintage.
        identifier: confidence level string e.g. '95%', '85%', '75%', '65%'.
        std_error_col: column name for std_error in imputed_table.
        n_samples: number of training samples (from artifact["n_training_samples"]).
        n_features: number of model features (from artifact["n_features"]).

    Returns:
        margin_of_error in millions (rounded to 2 decimals).
    """
    yearly_std_error = _compute_yearly_std_error(
        imputed_df, vintage_month, vintage_col, std_error_col
    )
    if yearly_std_error == 0.0:
        return 0.0

    if n_samples <= 0:
        n_samples = 1

    df_val = max(1, n_samples - n_features - 1)
    use_t = n_samples < 30

    conf_level = _IDENTIFIER_TO_CONF.get(identifier)
    if conf_level is None:
        pct = identifier.replace("%", "").strip()
        try:
            conf_level = float(pct) / 100.0
        except ValueError:
            return 0.0

    alpha = 1 - conf_level
    if use_t:
        critical_value = scipy_stats.t.ppf(1 - alpha / 2, df=df_val)
    else:
        critical_value = scipy_stats.norm.ppf(1 - alpha / 2)

    yearly_std_error_millions = yearly_std_error / 1e6
    margin_millions = round(critical_value * yearly_std_error_millions, 2)

    return margin_millions


def _feature_set_from_imputed(
    imputed_df: pd.DataFrame,
    vintage_month: str,
    vintage_col: str,
    target_month_col: str,
    features_list_col: Optional[str],
    feat_prefix: str,
    meta_cols: tuple[str, ...],
    history_df: Optional[pd.DataFrame] = None,
    month_id_col: str = "month_id",
    actual_filled_col: str = "actual_filled",
) -> list[dict[str, Any]]:
    """
    Build feature_set as list of dicts — full year view (Jan..Dec):
    - Months BEFORE vintage (e.g. Jan, Feb for vintage Mar): source="actual",
      with target_month, revenue_millions (from history_df actual_filled / 1e6).
    - Months FROM vintage onward (e.g. Mar..Dec): source="predicted",
      with target_month, revenue_millions, and all feat_* feature columns.
    """
    vy, vm = _month_id_to_year_month(vintage_month)

    result: list[dict[str, Any]] = []

    # --- Part 1: Actual months (Jan to vintage_month - 1) from history_df ---
    if history_df is not None and vm > 1:
        for m in range(1, vm):
            month_id = f"{vy}-{m:02d}"
            actual_item: dict[str, Any] = {
                "target_month": month_id,
                "source": "actual",
            }
            if month_id_col in history_df.columns and actual_filled_col in history_df.columns:
                mask_h = history_df[month_id_col].astype(str).str.strip() == month_id
                rows_h = history_df.loc[mask_h, actual_filled_col]
                if not rows_h.empty:
                    val = rows_h.apply(pd.to_numeric, errors="coerce").fillna(0).iloc[0]
                    actual_item["revenue_millions"] = round(float(val) / 1e6, 2)
                else:
                    actual_item["revenue_millions"] = None
            else:
                actual_item["revenue_millions"] = None
            result.append(actual_item)

    # --- Part 2: Predicted months (vintage_month to Dec) from imputed_table ---
    mask = imputed_df[vintage_col].astype(str).str.strip() == str(vintage_month).strip()
    subset = imputed_df.loc[mask].copy()

    if subset.empty:
        return result

    use_features_list = (
        features_list_col
        and features_list_col in subset.columns
        and subset[features_list_col].notna().any()
    )

    for _, row in subset.iterrows():
        item: dict[str, Any] = {}

        # target_month
        if target_month_col in row.index:
            item["target_month"] = str(row[target_month_col])

        # feat_* columns (strip prefix)
        for col in subset.columns:
            if col in meta_cols or col == target_month_col:
                continue
            if feat_prefix and col.startswith(feat_prefix):
                key = col[len(feat_prefix):]
            else:
                continue
            v = row[col]
            if pd.isna(v):
                item[key] = None
            elif isinstance(v, (int, float)):
                item[key] = float(v) if isinstance(v, float) else int(v)
            else:
                item[key] = v

        # If features_list_col is present and usable, merge it in
        if use_features_list:
            val = row[features_list_col]
            extra = {}
            if isinstance(val, (dict, list)):
                extra = dict(val) if isinstance(val, dict) else (val[0] if val and isinstance(val[0], dict) else {})
            elif isinstance(val, str):
                try:
                    parsed = json.loads(val)
                    extra = parsed if isinstance(parsed, dict) else {}
                except Exception:
                    extra = {}
            item.update(extra)

        # revenue_millions from predicted_revenue
        if "predicted_revenue" in row.index and pd.notna(row.get("predicted_revenue")):
            try:
                item["revenue_millions"] = round(float(row["predicted_revenue"]) / 1e6, 2)
            except Exception:
                pass

        # source marker
        item["source"] = "predicted"

        result.append(item)

    return result


def build_vintage_summary_rows(
    vintage_month: str,
    history_df: pd.DataFrame,
    imputed_table: pd.DataFrame,
    *,
    fpna_df: Optional[pd.DataFrame] = None,
    forecast_cols: Optional[list[str]] = None,
    entity_type: Optional[str] = None,
    entity_name: Optional[str] = None,
    time_dim: Optional[str] = None,
    identifiers: Optional[list[str]] = None,
    model_artifact: Optional[Union[Dict[str, Any], str]] = None,
    n_samples: int = 100,
    n_features: int = 10,
) -> list[dict[str, Any]]:
    """
    Build 4 summary rows for one vintage_month — one row per identifier (95%, 85%, 75%, 65%).

    - fpna_forecast: if max(month_id) in history_df == vintage_month and fpna_df is provided,
      use fpna_df (exclude Risk/Opportunity, sum Forecast_Jan..Forecast_Dec). Otherwise use
      sum of forecast_jan..forecast_dec from history_df for that vintage row.
    - absolute_model_forecast: sum of predicted_revenue (vintage..Dec) + previous months actual_filled.
    - margin_of_error: critical_value * yearly_std_error_millions per identifier (z/t distribution).
    - delta_forecast: fpna_forecast - absolute_model_forecast.
    - feature_set: list of dicts (one per target month from imputed_table).

    Args:
        model_artifact: either a dict (loaded artifact) or a str (path to .joblib file).
            If provided, n_samples and n_features are taken from:
                artifact["n_training_samples"] and artifact["n_features"].
        n_samples: fallback if model_artifact is not provided.
        n_features: fallback if model_artifact is not provided.

    Returns:
        List of 4 dicts (one per identifier).
    """
    cfg_h = CONFIG["history"]
    cfg_i = CONFIG["imputed"]
    cfg_o = CONFIG["output"]
    cfg_fpna = CONFIG.get("fpna_abfs", {})

    # --- Extract n_samples and n_features from model artifact ---
    if model_artifact is not None:
        if isinstance(model_artifact, str):
            model_artifact = load_model_artifact(model_artifact)
        n_samples = model_artifact.get("n_training_samples") or n_samples
        n_features = model_artifact.get("n_features") or n_features

    if identifiers is None:
        identifiers = list(IDENTIFIERS)

    month_id_col = cfg_h.get("month_id_col")
    use_fpna_df = False
    if month_id_col and month_id_col in history_df.columns:
        max_month_id = history_df[month_id_col].astype(str).str.strip().max()
        if str(vintage_month).strip() == max_month_id and fpna_df is not None:
            use_fpna_df = True

    if use_fpna_df:
        fpna = fpna_forecast_from_abfs_df(
            fpna_df,
            vintage_month=vintage_month,
            month_id_col=cfg_fpna.get("month_id_col"),
            revenue_category_col=cfg_fpna.get("revenue_category_col", "Revenue_Category"),
            exclude_categories=cfg_fpna.get("exclude_categories", ("Risk", "Opportunity")),
            forecast_col_pattern=cfg_fpna.get("forecast_col_pattern"),
            forecast_col_flags=cfg_fpna.get("forecast_col_flags", re.IGNORECASE),
        )
    else:
        if forecast_cols is None:
            forecast_cols = _get_forecast_columns(
                history_df,
                cfg_h["forecast_col_pattern"],
                cfg_h["forecast_col_flags"],
            )
        if not forecast_cols:
            raise ValueError(
                "No forecast columns found in history_df. "
                "Check CONFIG['history']['forecast_col_pattern'] or pass forecast_cols."
            )
        fpna = _fpna_forecast_from_history(
            history_df,
            vintage_month,
            forecast_cols,
            cfg_h["vintage_year_col"],
            cfg_h["vintage_month_col"],
            month_id_col,
        )

    abs_model = compute_absolute_model_forecast(
        imputed_table,
        history_df,
        vintage_month,
        vintage_col=cfg_i["vintage_col"],
        target_month_col=cfg_i["target_month_col"],
        predicted_revenue_col=cfg_i["predicted_revenue_col"],
        month_id_col=cfg_h.get("month_id_col") or "month_id",
        actual_filled_col="actual_filled",
    )
    delta = fpna - abs_model

    feature_set_list = _feature_set_from_imputed(
        imputed_table,
        vintage_month,
        cfg_i["vintage_col"],
        cfg_i["target_month_col"],
        cfg_i.get("features_list_col"),
        cfg_i["feat_prefix"],
        cfg_i["meta_cols"],
        history_df=history_df,
        month_id_col=cfg_h.get("month_id_col") or "month_id",
        actual_filled_col="actual_filled",
    )
    feature_set_json = json.dumps(feature_set_list, indent=2)

    vintage_year, vintage_month_num = _month_id_to_year_month(vintage_month)

    rows = []
    for ident in identifiers:
        margin = _margin_of_error_for_identifier(
            imputed_table,
            vintage_month,
            cfg_i["vintage_col"],
            ident,
            std_error_col=cfg_i["std_error_col"],
            n_samples=n_samples,
            n_features=n_features,
        )
        rows.append({
            "year": vintage_year,
            "month": vintage_month_num,
            "entity_type": entity_type if entity_type is not None else cfg_o["entity_type"],
            "entity_name": entity_name if entity_name is not None else cfg_o["entity_name"],
            "time_dim": time_dim if time_dim is not None else cfg_o["time_dim"],
            "identifier": ident,
            "fpna_forecast": round(fpna / 1e6, 2),
            "absolute_model_forecast": round(abs_model / 1e6, 2),
            "margin_of_error": round(margin, 2),
            "delta_forecast": round(delta / 1e6, 2),
            "feature_set": feature_set_json,
        })

    return rows


def build_vintage_summary_df(
    history_df: pd.DataFrame,
    imputed_table: pd.DataFrame,
    vintage_month: str,
    *,
    fpna_df: Optional[pd.DataFrame] = None,
    forecast_cols: Optional[list[str]] = None,
    entity_type: Optional[str] = None,
    entity_name: Optional[str] = None,
    time_dim: Optional[str] = None,
    identifiers: Optional[list[str]] = None,
    model_artifact: Optional[Union[Dict[str, Any], str]] = None,
    n_samples: int = 100,
    n_features: int = 10,
) -> pd.DataFrame:
    """
    Build a DataFrame with 4 rows (one per identifier: 95%, 85%, 75%, 65%)
    for a single vintage_month (string, e.g. '2026-01').

    Args:
        history_df: historical data with month_id, actual_filled.
        imputed_table: inference table with vintage, target_month, predicted_revenue, etc.
        vintage_month: single month string e.g. '2026-01'.
        fpna_df: ABFS raw data for fpna_forecast.
        model_artifact: dict or path to .joblib for n_training_samples, n_features.
        n_samples: fallback if model_artifact not provided.
        n_features: fallback if model_artifact not provided.
    """
    rows = build_vintage_summary_rows(
        vintage_month,
        history_df,
        imputed_table,
        fpna_df=fpna_df,
        forecast_cols=forecast_cols,
        entity_type=entity_type,
        entity_name=entity_name,
        time_dim=time_dim,
        identifiers=identifiers,
        model_artifact=model_artifact,
        n_samples=n_samples,
        n_features=n_features,
    )
    return pd.DataFrame(rows)


# --- Example usage ---
if __name__ == "__main__":
    
    artifact = artifact
    
    # --- Load data ---
    history_df = historical_df
    imputed_table = imputed_table
    fpna_df = spark.read.option("header", "true").option("inferSchema", "true").csv(full_path).toPandas()

    # --- Build summary (4 rows per vintage: 95%, 85%, 75%, 65%) ---
    forecast_result_df = build_vintage_summary_df(
        history_df, imputed_table,
        vintage_month=month_id,
        fpna_df=fpna_df,
        model_artifact=artifact,
    )
    display(forecast_result)

###########################################
##upsert logic for table above

from delta.tables import DeltaTable

# Prepare source and target
source_df = spark.createDataFrame(forecast_result_df)  # result is your source DataFrame
display(source_df)
target_table = "fpnacopilot.data_engineering.enterprise_forecast_output"
display(target_table)

# Get DeltaTable object for the target table
delta_table = DeltaTable.forName(spark, target_table)

# Define merge condition on year and month
merge_condition = "target.year = source.year AND target.month = source.month AND target.identifier = source.identifier" 

# Prepare update and insert mappings for all columns
update_set = {col: f"source.{col}" for col in source_df.columns}
insert_set = {col: f"source.{col}" for col in source_df.columns}

# Perform upsert (merge) operation:
# - If a row with the same year and month exists, update all columns with source values
# - If not, insert the new row from source
(
    delta_table.alias("target")
    .merge(
        source_df.alias("source"),
        merge_condition
    )
    .whenMatchedUpdate(set=update_set)
    .whenNotMatchedInsert(values=insert_set)
    .execute()
)

# Display the updated target table
display(spark.read.table(target_table))


###### load data and model


import json
# import logging
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass

import numpy as np
import pandas as pd

# Configure logging
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# )
# logger = logging.getLogger(__name__)

# =============================================================================
# Configuration
# =============================================================================


@dataclass
class InterpretabilityConfig:
    """Configuration for SHAP and LIME analysis."""
    shap_max_samples: int = 100  # Max samples for SHAP background
    shap_explainer_type: str = "auto"  # auto, tree, kernel, linear
    lime_num_features: int = 10
    lime_num_samples: int = 5000
    top_n_features: int = 10  # Top N features to include in summary


# =============================================================================
# Model Loading
# =============================================================================

def load_model_and_data():
    """
    PLACEHOLDER: Replace this function with your actual model and data loading code.
    
    Returns:
        model: Trained ML model object
        model_type: String name of model type (e.g., "RandomForestRegressor")
        feature_names: List of feature names
        preprocessor: Optional preprocessor/transformer pipeline
        feature_selector: Optional feature selector
        X_train: Training features (numpy array or DataFrame)
        X_explain: Features to explain (numpy array or DataFrame)
        y_train: Training target (optional)
        y_explain: Target to explain (optional)
        df_data: Full DataFrame with data for statistics
        target_col: Name of target column
        model_metrics: Dict with 'rmse', 'r2', 'mape' keys
        months_lookback: Integer, number of months analyzed
    """
    # TODO: Replace this placeholder with your actual code
    # Example structure:
    model = artifact["model"]
    model_type = type(model).__name__
    feature_names = artifact['feature_names']
    
    raw_feature_names = [col.replace('recip_', '') for col in feature_names]
    preprocessor = artifact['preprocessor']
    
    #X_transformed = preprocessor.transform(X_explain)


    ##train_mask = historical_df["year"].isin(trained_year.iloc[0])###
    train_mask = historical_df["year"].isin(trained_year.iloc[0].tolist())
    test_mask  = historical_df["year"].isin(test_year.iloc[0].tolist())
    # train_mask = historical_df["year"].isin([2023,2024,2025])
    # test_mask  = historical_df["year"].isin([2026])
    inference_mask = historical_df["year"]==inference_year
    

    X_train = historical_df.loc[train_mask, raw_feature_names].fillna(0)
    X_train_transformed=preprocessor.transform(X_train)
    
    y_train = historical_df.loc[train_mask, "revenue"]

    X_explain_row_mask = ((historical_df['year'] == inference_year) & (historical_df['month'] == int(inference_month)))
    # y_explain_row_mask = ((imputed_table['year'] == inference_year) & (imputed_table['month'] == int(inference_month)))
    y_explain_row_mask = ((imputed_table['vintage'].str[:4] == year) & (imputed_table['vintage'].str[-2:] == month))


    X_explain = (historical_df.loc[X_explain_row_mask, raw_feature_names].fillna(0))
    X_explain_raw = X_explain.copy()  # Keep raw (untransformed) for LIME original-scale display 

    X_explain_transformed = preprocessor.transform(X_explain)

    #X_explain = historical_df[X_explain_row_mask][feature_names]
    y_explain = imputed_table[y_explain_row_mask]["predicted_revenue"]

    
    #X_explain =  historical_df['revenue'] #pipe.X_test_processed # Data to explain

    df_data = historical_df[raw_feature_names].fillna(0) # Full DataFrame
    model_metrics = None #artifact['metrics']
    target_col = "revenue"
    
    feature_selector = None
    # y_train = historical_df['revenue']
    # y_explain =  historical_df['revenue'] 
    months_lookback = len(historical_df[historical_df['year'] == (inference_year - 1)])


    return  model, model_type, feature_names, preprocessor, feature_selector,X_train_transformed, X_explain_transformed, y_train, y_explain,df_data, target_col, model_metrics, months_lookback, X_explain_raw

    
    raise NotImplementedError(
        "Please replace this function with your actual model and data loading code."
    )



      
