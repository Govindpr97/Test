{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee5481e6-d784-4ba7-bc97-f12f318e5c8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "access_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ea3a713-9f75-4199-a032-2c82b1f0f7a1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read the final dummy data"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"fpnacopilot.data_engineering.final_dummy_data\"\n",
    "#df = pd.read_csv(\"/Workspace/final_imputed.csv\")\n",
    "df = spark.read.table(file_path)\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83ec00d8-fb46-4450-89dd-792997fda60d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 3"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "sys.path.insert(0, '/Workspace/Notebook&Files')\n",
    "\n",
    "import config\n",
    "\n",
    "print(\"Config imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf804e4d-ccb1-4bed-a933-b147c799ff02",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 4"
    }
   },
   "outputs": [],
   "source": [
    "print(config.FULL_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "693425e9-a1ce-4711-8f14-734a5b406e3f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "logger"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from io import BytesIO\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Azure Storage account details\n",
    "storage_account_name = \"nistorage\"\n",
    "container_name = \"dbnotebook-logs\"  # Container for logs\n",
    "\n",
    "# Connection string for Azure Blob Storage\n",
    "connection_string = (\n",
    "    f\"DefaultEndpointsProtocol=https;\"\n",
    "    f\"AccountName={storage_account_name};\"\n",
    "    f\"AccountKey={access_key};\"\n",
    "    f\"EndpointSuffix=core.windows.net\"\n",
    ")\n",
    "\n",
    "# Custom logging handler for Azure Blob Storage\n",
    "class AzureBlobHandler(logging.Handler):\n",
    "    \"\"\"\n",
    "    Custom logging handler that writes log records to an Azure Blob Storage file.\n",
    "    Inherits from logging.Handler, which is the base class for all log handlers in Python's logging module.\n",
    "    Each time a log record is emitted, it appends the formatted log entry to the specified blob.\n",
    "    \"\"\"\n",
    "    def __init__(self, connection_string, container_name, blob_name):\n",
    "        super().__init__()\n",
    "        self.blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "        self.container_name = container_name\n",
    "        self.blob_name = blob_name\n",
    "        \n",
    "    def emit(self, record):\n",
    "        \"\"\"\n",
    "        Emit a log record.\n",
    "        Downloads the current blob content (if exists), appends the new log entry, and uploads it back.\n",
    "        If the blob does not exist, it creates a new one.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            log_entry = self.format(record) + '\\n'\n",
    "            blob_client = self.blob_service_client.get_blob_client(\n",
    "                container=self.container_name, \n",
    "                blob=self.blob_name\n",
    "            )\n",
    "            try:\n",
    "                existing_data = blob_client.download_blob().readall().decode('utf-8')\n",
    "                new_data = existing_data + log_entry\n",
    "            except Exception:\n",
    "                new_data = log_entry\n",
    "            blob_client.upload_blob(new_data.encode('utf-8'), overwrite=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing log to blob: {e}\")\n",
    "\n",
    "# Create logger\n",
    "file_date = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d')\n",
    "blob_log_name = f'logs_model/custom_log_{file_date}.log'\n",
    "print(f\"Logging to: wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{blob_log_name}\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# âœ… FIX: Clear all existing handlers to prevent duplicate logs\n",
    "logger.handlers.clear()\n",
    "\n",
    "handler = AzureBlobHandler(connection_string, container_name, blob_log_name)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "print(f\"Logger configured with {len(logger.handlers)} handler(s)\")\n",
    "\n",
    "# Test log\n",
    "logger.info('Logger initialized and writing to Azure Blob Storage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba4af7c9-f59b-491a-9f35-885c5a6e0dea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 6"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Training Script for Revenue Forecasting - Databricks Unity Catalog\n",
    "\"\"\"\n",
    "\n",
    "#import logging\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# import config\n",
    "\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO,\n",
    "#     format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "#     datefmt='%Y-%m-%d %H:%M:%S'\n",
    "# )\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"Creates features for revenue prediction\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_list = config.ALL_FEATURES\n",
    "    \n",
    "    def create_features(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "        logger.info(f\"[FEATURE] Creating features for {len(df)} rows\")\n",
    "        \n",
    "        try:\n",
    "            df_feat = df.copy().sort_values(['year', 'month_num']).reset_index(drop=True)\n",
    "            \n",
    "            if 'month_id' not in df_feat.columns:\n",
    "                df_feat['month_id'] = df_feat['year'] * 100 + df_feat['month_num']\n",
    "            \n",
    "            # Static/Calendar\n",
    "            df_feat['remaining_months'] = 13 - df_feat['month_num']\n",
    "            df_feat['quarter'] = ((df_feat['month_num'] - 1) // 3) + 1\n",
    "            df_feat['is_q4'] = (df_feat['quarter'] == 4).astype(int)\n",
    "            df_feat['is_q2'] = (df_feat['quarter'] == 2).astype(int)\n",
    "            df_feat['is_end_of_quarter'] = df_feat['month_num'].isin([3, 6, 9, 12]).astype(int)\n",
    "            df_feat['is_quarter_start'] = df_feat['month_num'].isin([1, 4, 7, 10]).astype(int)\n",
    "            df_feat['quarter_position'] = ((df_feat['month_num'] - 1) % 3) + 1\n",
    "            \n",
    "            # Past Year\n",
    "            df_feat['ly_same_month_revenue'] = df_feat.groupby('month_num')['actual_revenue'].shift(1)\n",
    "            df_feat['ly_same_qtr_avg'] = df_feat.groupby(['quarter'])['actual_revenue'].transform(\n",
    "                lambda x: x.shift(3).rolling(3, min_periods=1).mean()\n",
    "            )\n",
    "            \n",
    "            # Lag Features\n",
    "            df_feat['revenue_lag_1'] = df_feat['actual_revenue'].shift(1)\n",
    "            df_feat['revenue_lag_2'] = df_feat['actual_revenue'].shift(2)\n",
    "            df_feat['revenue_lag_3'] = df_feat['actual_revenue'].shift(3)\n",
    "            df_feat['revenue_3mo_avg'] = df_feat['actual_revenue'].shift(1).rolling(3, min_periods=1).mean()\n",
    "            df_feat['revenue_velocity'] = df_feat['revenue_lag_1'] - df_feat['revenue_lag_2']\n",
    "            df_feat['revenue_acceleration'] = df_feat['revenue_velocity'] - df_feat['revenue_velocity'].shift(1)\n",
    "            \n",
    "            # YTD\n",
    "            df_feat['ytd_revenue'] = df_feat.groupby('year')['actual_revenue'].cumsum().shift(1)\n",
    "            df_feat['ytd_avg'] = df_feat['ytd_revenue'] / (df_feat['month_num'] - 1).replace(0, 1)\n",
    "            df_feat['perf_vs_ytd'] = ((df_feat['revenue_lag_1'] - df_feat['ytd_avg']) / \n",
    "                                      (df_feat['ytd_avg'] + 1e-10)).clip(-0.5, 0.5)\n",
    "            \n",
    "            # Quarter\n",
    "            df_feat['quarter_cumulative'] = df_feat.groupby(['year', 'quarter'])['actual_revenue'].cumsum().shift(1)\n",
    "            df_feat['prev_quarter_avg'] = df_feat['actual_revenue'].shift(1).rolling(3, min_periods=1).mean().shift(2)\n",
    "            df_feat['last_quarter_end_rev'] = df_feat['actual_revenue'].shift(1).where(\n",
    "                df_feat['month_num'].shift(1).isin([3, 6, 9, 12])\n",
    "            ).ffill()\n",
    "            df_feat['qoq_change'] = ((df_feat['revenue_3mo_avg'] - df_feat['prev_quarter_avg']) / \n",
    "                                    (df_feat['prev_quarter_avg'] + 1e-10)).clip(-0.5, 0.5)\n",
    "            \n",
    "            # Trend\n",
    "            df_feat['revenue_6mo_avg'] = df_feat['actual_revenue'].shift(1).rolling(6, min_periods=1).mean()\n",
    "            df_feat['trend_direction'] = np.sign(df_feat['revenue_3mo_avg'] - df_feat['revenue_6mo_avg'])\n",
    "            \n",
    "            # Forecast Remaining\n",
    "            df_feat['fcst_total_rem'] = (df_feat['committed_sign_revenue'] + \n",
    "                                          df_feat['committed_unsig_revenue'] + \n",
    "                                          df_feat['wtd_pipeline_revenue'])\n",
    "            df_feat['fcst_signed_rem'] = df_feat['committed_sign_revenue']\n",
    "            df_feat['fcst_unsigned_rem'] = df_feat['committed_unsig_revenue']\n",
    "            df_feat['fcst_pipeline_rem'] = df_feat['wtd_pipeline_revenue']\n",
    "            df_feat['signed_per_month'] = df_feat['fcst_signed_rem'] / df_feat['remaining_months'].replace(0, 1)\n",
    "            \n",
    "            # Ratios\n",
    "            df_feat['committed_ratio'] = df_feat['fcst_signed_rem'] / (df_feat['fcst_total_rem'] + 1e-10)\n",
    "            df_feat['unsigned_ratio'] = df_feat['fcst_unsigned_rem'] / (df_feat['fcst_total_rem'] + 1e-10)\n",
    "            df_feat['pipeline_quality'] = (\n",
    "                df_feat['fcst_signed_rem'] * 1.0 + \n",
    "                df_feat['fcst_unsigned_rem'] * 0.7 +\n",
    "                df_feat['fcst_pipeline_rem'] * 0.3\n",
    "            ) / (df_feat['fcst_total_rem'] + 1e-10)\n",
    "            \n",
    "            # Handle infinities\n",
    "            for col in df_feat.columns:\n",
    "                if df_feat[col].dtype in [np.float64, np.float32]:\n",
    "                    df_feat[col] = df_feat[col].replace([np.inf, -np.inf], np.nan)\n",
    "            \n",
    "            logger.info(f\"[FEATURE] Created {len(self.feature_list)} features\")\n",
    "            return df_feat, self.feature_list\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"[FEATURE] Error: {str(e)}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            raise\n",
    "\n",
    "\n",
    "class RevenueModelTrainer:\n",
    "    \"\"\"Revenue Forecasting Model Trainer\"\"\"\n",
    "    \n",
    "    def __init__(self, request_id: str = None):\n",
    "        self.request_id = request_id or datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.feature_engineer = FeatureEngineer()\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.selected_features = config.FINAL_FEATURES\n",
    "        self.feature_medians = None\n",
    "        self.metrics = {}\n",
    "        self.X_train = None\n",
    "        self.run_id = None\n",
    "        \n",
    "        logger.info(f\"[{self.request_id}] Trainer initialized\")\n",
    "    \n",
    "    def load_data(self, df: pd.DataFrame = None, file_path: str = None) -> pd.DataFrame:\n",
    "        logger.info(f\"[{self.request_id}] Loading data\")\n",
    "        \n",
    "        if df is not None:\n",
    "            data = df.copy()\n",
    "        elif file_path:\n",
    "        \n",
    "            data = spark.read.table(file_path).toPandas()\n",
    "            logger.info(f\"[{self.request_id}] File: {file_path}\")\n",
    "            # data = pd.read_csv(file_path, index_col=0)\n",
    "            # logger.info(f\"[{self.request_id}] File: {file_path}\")\n",
    "        else:\n",
    "            raise ValueError(\"Provide df or file_path\")\n",
    "        \n",
    "        data = data.sort_values(['year', 'month_num']).reset_index(drop=True)\n",
    "        data['month_id'] = data['year'] * 100 + data['month_num']\n",
    "        \n",
    "        logger.info(f\"[{self.request_id}] Rows: {len(data)}, Years: {sorted(data['year'].unique())}\")\n",
    "        return data\n",
    "    \n",
    "    def prepare_data(self, df: pd.DataFrame, train_years: List[int] = None, test_year: int = None):\n",
    "        logger.info(f\"[{self.request_id}] Preparing data\")\n",
    "        \n",
    "        df_feat, _ = self.feature_engineer.create_features(df)\n",
    "        \n",
    "        years = sorted(df_feat['year'].unique())\n",
    "        train_years = train_years or years[:-1]\n",
    "        test_year = test_year or years[-1]\n",
    "        \n",
    "        train_df = df_feat[df_feat['year'].isin(train_years)].dropna(subset=[config.TARGET])\n",
    "        test_df = df_feat[df_feat['year'] == test_year].copy()\n",
    "        \n",
    "        logger.info(f\"[{self.request_id}] Train: {len(train_df)} ({train_years}), Test: {len(test_df)} ({test_year})\")\n",
    "        return df_feat, train_df, test_df\n",
    "    \n",
    "    def train(self, df: pd.DataFrame = None, file_path: str = None) -> Dict[str, float]:\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        logger.info(f\"[{self.request_id}] ========== TRAINING STARTED ==========\")\n",
    "        \n",
    "        try:\n",
    "            data = self.load_data(df, file_path)\n",
    "            ##################\n",
    "            # display(data)\n",
    "            df_feat, train_df, test_df = self.prepare_data(data)\n",
    "            # display(df_feat)\n",
    "            # display(train_df)\n",
    "            # display(test_df)\n",
    "            X_test = test_df[self.selected_features].copy()\n",
    "            y_test = test_df[config.TARGET].copy\n",
    "            display(X_test)\n",
    "\n",
    "            display(y_test)\n",
    "            \n",
    "            X_train = train_df[self.selected_features].copy()\n",
    "            y_train = train_df[config.TARGET].copy()\n",
    "            full_data = df_feat[self.selected_features + [config.TARGET]].copy()\n",
    "            \n",
    "            spark.createDataFrame(X_test).write.mode(\"overwrite\").saveAsTable(\"fpnacopilot.machine_learning.testfeaturedata\")\n",
    "            spark.createDataFrame(X_train).write.mode(\"overwrite\").saveAsTable(\"fpnacopilot.machine_learning.trainfeaturedata\")\n",
    "            spark.createDataFrame(full_data).write.mode(\"overwrite\").saveAsTable(\"fpnacopilot.machine_learning.fullfeaturedata\")\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            self.feature_medians = X_train.median()\n",
    "            X_train = X_train.fillna(self.feature_medians)\n",
    "            self.X_train = X_train.copy()\n",
    "            \n",
    "            self.scaler = StandardScaler()\n",
    "            X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "            \n",
    "            tscv = TimeSeriesSplit(n_splits=config.CV_SPLITS)\n",
    "            alphas = np.logspace(-1, 4, 50)\n",
    "            \n",
    "            self.model = RidgeCV(alphas=alphas, cv=tscv)\n",
    "            self.model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            logger.info(f\"[{self.request_id}] Model trained - Alpha: {self.model.alpha_:.2f}\")\n",
    "            \n",
    "            # Metrics\n",
    "            train_pred = self.model.predict(X_train_scaled)\n",
    "            self.metrics = {\n",
    "                'mape': mean_absolute_percentage_error(y_train, train_pred) * 100,\n",
    "                'mae': mean_absolute_error(y_train, train_pred),\n",
    "                'rmse': np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"[{self.request_id}] MAPE: {self.metrics['mape']:.2f}%\")\n",
    "            logger.info(f\"[{self.request_id}] ========== TRAINING COMPLETED ==========\")\n",
    "            \n",
    "            return self.metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"[{self.request_id}] Training error: {str(e)}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            raise\n",
    "    \n",
    "    def register_to_unity_catalog(self) -> str:\n",
    "        \"\"\"\n",
    "        Register model to Unity Catalog\n",
    "        \n",
    "        Uses: mlflow.set_registry_uri(\"databricks-uc\")\n",
    "        Model path: {catalog}.{schema}.{model_name}\n",
    "        \"\"\"\n",
    "        logger.info(f\"[{self.request_id}] ========== REGISTERING TO UNITY CATALOG ==========\")\n",
    "        logger.info(f\"[{self.request_id}] Model: {config.FULL_MODEL_NAME}\")\n",
    "        \n",
    "        try:\n",
    "            # Set Unity Catalog registry\n",
    "            mlflow.set_registry_uri(\"databricks-uc\")\n",
    "            \n",
    "            with mlflow.start_run() as run:\n",
    "                # Create signature\n",
    "                signature = infer_signature(self.X_train, self.model.predict(self.scaler.transform(self.X_train)))\n",
    "                \n",
    "                # Log model\n",
    "                mlflow.sklearn.log_model(\n",
    "                    sk_model=self.model,\n",
    "                    artifact_path=\"model\",\n",
    "                    signature=signature,\n",
    "                    input_example=self.X_train.head(5)\n",
    "                )\n",
    "                \n",
    "                # Log scaler\n",
    "                mlflow.sklearn.log_model(\n",
    "                    sk_model=self.scaler,\n",
    "                    artifact_path=\"scaler\"\n",
    "                )\n",
    "                \n",
    "                # Log metrics\n",
    "                for key, val in self.metrics.items():\n",
    "                    mlflow.log_metric(key, val)\n",
    "                \n",
    "                # Log params\n",
    "                mlflow.log_param(\"alpha\", self.model.alpha_)\n",
    "                mlflow.log_param(\"n_features\", len(self.selected_features))\n",
    "                mlflow.log_param(\"features\", str(self.selected_features))\n",
    "                \n",
    "                # Log artifacts\n",
    "                mlflow.log_dict({\"features\": self.selected_features}, \"feature_names.json\")\n",
    "                mlflow.log_dict(self.feature_medians.to_dict(), \"feature_medians.json\")\n",
    "                \n",
    "                # Register to Unity Catalog\n",
    "                model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "                mlflow.register_model(model_uri=model_uri, name=config.FULL_MODEL_NAME)\n",
    "                \n",
    "                self.run_id = run.info.run_id\n",
    "            \n",
    "            logger.info(f\"[{self.request_id}] Run ID: {self.run_id}\")\n",
    "            logger.info(f\"[{self.request_id}] Registered to: {config.FULL_MODEL_NAME}\")\n",
    "            logger.info(f\"[{self.request_id}] ========== REGISTRATION COMPLETED ==========\")\n",
    "            \n",
    "            return self.run_id\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"[{self.request_id}] Registration error: {str(e)}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            raise\n",
    "    \n",
    "    def train_and_register(self, df: pd.DataFrame = None, file_path: str = None) -> Dict:\n",
    "        \"\"\"Train and register in one step\"\"\"\n",
    "        self.train(df=df, file_path=file_path)\n",
    "        run_id = self.register_to_unity_catalog()\n",
    "        \n",
    "        return {\n",
    "            'run_id': run_id,\n",
    "            'model_name': config.FULL_MODEL_NAME,\n",
    "            'metrics': self.metrics\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "855c4fba-cca8-4780-b7fc-2dfd34a608c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# In Databricks notebook:\n",
    "\n",
    "#from model_training import RevenueModelTrainer\n",
    "\n",
    "\n",
    "# Option 1: Train and register in one step\n",
    "trainer = RevenueModelTrainer(request_id=\"training_001\")\n",
    "\n",
    "result = trainer.train_and_register(file_path = config.file_path)\n",
    "\n",
    "print(f\"Run ID: {result['run_id']}\")\n",
    "print(f\"Model: {result['model_name']}\")\n",
    "print(f\"MAPE: {result['metrics']['mape']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96794a0a-0360-4f62-b709-9f3b8053f2b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(model.predict([[2140805032.76,\t12,\t508515179.11,\t-0.5,\t1,\t127898772.13,\t-94471806.81,\t222370578.94,\t-158596557.71,\t71536376.66416667,\t-0.0747306702941992,\t0,\t-1,\t158245828.04,\t0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e25a85f6-3e8a-44de-94b0-6fd97020a60c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "testing logging manually"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import uuid\n",
    "import json\n",
    "import mlflow\n",
    "\n",
    "# Example inference input and output\n",
    "databricks_request_id = str(uuid.uuid4())\n",
    "request_time = datetime.utcnow().isoformat()\n",
    "request = {\n",
    "    \"features\": [2140805032.76, 12, 508515179.11, -0.5, 1, 127898772.13, -94471806.81, 222370578.94, -158596557.71, 71536376.66416667, -0.0747306702941992, 0, -1, 158245828.04, 0]\n",
    "}\n",
    "\n",
    "model = mlflow.sklearn.load_model(\"models:/fpnacopilot.ml_model.revenue_forecasting_model_v2/1\")\n",
    "                                \n",
    "try:\n",
    "    prediction = model.predict([request[\"features\"]]).tolist()\n",
    "    response = {\"prediction\": prediction}\n",
    "    status_code = 200\n",
    "except Exception as e:\n",
    "    response = {\"error\": str(e)}\n",
    "    # Example: set 400 for bad request, 403 for forbidden, etc. Here, default to 400 for inference error\n",
    "    status_code = 400\n",
    "\n",
    "# Prepare as Spark DataFrame\n",
    "log_row = [(databricks_request_id, request_time, status_code, json.dumps(request), json.dumps(response))]\n",
    "columns = [\"databricks_request_id\", \"request_time\", \"status_code\", \"request\", \"response\"]\n",
    "log_df = spark.createDataFrame(log_row, columns)\n",
    "\n",
    "# Append to Delta table (create if not exists)\n",
    "log_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"fpnacopilot.machine_learning.inference_log\")\n",
    "\n",
    "display(log_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8bc66f2-3145-43aa-ac31-1553fa5767bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "# import time, json, uuid\n",
    "# from datetime import datetime\n",
    " \n",
    "# def predict_with_mlflow_logging(model, df):\n",
    "#     request_id = str(uuid.uuid4())\n",
    "#     user = spark.sql(\"select current_user()\").collect()[0][0]\n",
    " \n",
    "#     with mlflow.start_run(run_name=f\"inference_{request_id}\", nested=True):\n",
    "#         start = time.time()\n",
    "#         preds = model.predict(df)\n",
    "#         end = time.time()\n",
    " \n",
    "#         # Log metadata\n",
    "#         mlflow.set_tag(\"request_id\", request_id)\n",
    "#         mlflow.set_tag(\"requested_by\", user)\n",
    " \n",
    "#         # Log metrics\n",
    "#         mlflow.log_metric(\"latency_ms\", int((end - start) * 1000))\n",
    "#         mlflow.log_metric(\"rows\", len(df))\n",
    " \n",
    "#         # Log payloads as artifacts\n",
    "#         mlflow.log_text(json.dumps(df), \"input.json\")\n",
    "#         mlflow.log_text(json.dumps(preds.tolist()), \"output.json\")\n",
    " \n",
    "#     return preds\n",
    " \n",
    " \n",
    "# model = mlflow.sklearn.load_model(\"models:/fpnacopilot.ml_model.revenue_forecasting_model_v2/1\")\n",
    " \n",
    "# preds = predict_with_mlflow_logging(model, [[2140805032.76, 12, 508515179.11, -0.5, 1, 127898772.13, -94471806.81, 222370578.94, -158596557.71, 71536376.66416667, -0.0747306702941992, 0, -1, 158245828.04, 0]])\n",
    "# print(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a00c36b-7f9f-42e4-99ef-aa70061a92e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "[2140805032.76,\t12,\t508515179.11,\t-0.5,\t1,\t127898772.13,\t-94471806.81,\t222370578.94,\t-158596557.71,\t71536376.66416667,\t-0.0747306702941992,\t0,\t-1,\t158245828.04,\t0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27fe6dd0-1ef6-49cb-a76c-2ab49c0ef85d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Model Registry"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Loading Utilities for Inference - Unity Catalog\n",
    "\"\"\"\n",
    "\n",
    "#import logging\n",
    "import json\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "#import config\n",
    "\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO,\n",
    "#     format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "#     datefmt='%Y-%m-%d %H:%M:%S'\n",
    "# )\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def load_model(version: str = None):\n",
    "    \"\"\"\n",
    "    Load model from Unity Catalog\n",
    "    \n",
    "    Args:\n",
    "        version: Model version (latest if None)\n",
    "    \n",
    "    Returns:\n",
    "        Loaded sklearn model\n",
    "    \"\"\"\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "    \n",
    "    if version:\n",
    "        model_uri = f\"models:/{config.FULL_MODEL_NAME}/{version}\"\n",
    "    else:\n",
    "        model_uri = f\"models:/{config.FULL_MODEL_NAME}@latest\"\n",
    "    \n",
    "    logger.info(f\"Loading model: {model_uri}\")\n",
    "    model = mlflow.sklearn.load_model(model_uri)\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def load_scaler(run_id: str):\n",
    "    \"\"\"Load scaler from run\"\"\"\n",
    "    scaler_uri = f\"runs:/{run_id}/scaler\"\n",
    "    \n",
    "    logger.info(f\"Loading scaler from run: {run_id}\")\n",
    "    scaler = mlflow.sklearn.load_model(scaler_uri)\n",
    "    \n",
    "    return scaler\n",
    "\n",
    "\n",
    "def load_artifacts(run_id: str):\n",
    "    \"\"\"Load feature names and medians\"\"\"\n",
    "    client = MlflowClient()\n",
    "    \n",
    "    path = client.download_artifacts(run_id, \"feature_names.json\")\n",
    "    with open(path, 'r') as f:\n",
    "        features = json.load(f)['features']\n",
    "    \n",
    "    path = client.download_artifacts(run_id, \"feature_medians.json\")\n",
    "    with open(path, 'r') as f:\n",
    "        medians = json.load(f)\n",
    "    \n",
    "    logger.info(f\"Loaded {len(features)} features\")\n",
    "    return features, medians\n",
    "\n",
    "\n",
    "def list_versions():\n",
    "    \"\"\"List all model versions\"\"\"\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "    client = MlflowClient()\n",
    "    \n",
    "    versions = client.search_model_versions(f\"name='{config.FULL_MODEL_NAME}'\")\n",
    "    \n",
    "    print(f\"\\nModel: {config.FULL_MODEL_NAME}\")\n",
    "    print(\"-\" * 50)\n",
    "    for v in versions:\n",
    "        print(f\"Version {v.version}: Run={v.run_id[:8]}...\")\n",
    "    \n",
    "    return versions\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATABRICKS NOTEBOOK USAGE (FOR INFERENCE)\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a492a22-f457-447a-a1da-7892f799d5d7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Inference"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Feature names in the correct order (must match model training)\n",
    "FEATURE_NAMES = [\n",
    "    'ytd_revenue',\n",
    "    'remaining_months', \n",
    "    'quarter_cumulative',\n",
    "    'perf_vs_ytd',\n",
    "    'is_quarter_start',\n",
    "    'revenue_lag_1',\n",
    "    'revenue_velocity',\n",
    "    'revenue_lag_2',\n",
    "    'revenue_acceleration',\n",
    "    'signed_per_month',\n",
    "    'qoq_change',\n",
    "    'is_q2',\n",
    "    'trend_direction',\n",
    "    'revenue_lag_3',\n",
    "    'is_q4'\n",
    "]\n",
    "\n",
    "\n",
    "def create_dataframe_records(data):\n",
    "    \"\"\"\n",
    "    Convert list of lists to dataframe_records format with column names.\n",
    "    This is required for MLflow model serving endpoints.\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        return data[FEATURE_NAMES].to_dict(orient='records')\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        df = pd.DataFrame(data, columns=FEATURE_NAMES)\n",
    "        return df.to_dict(orient='records')\n",
    "    elif isinstance(data, list):\n",
    "        df = pd.DataFrame(data, columns=FEATURE_NAMES)\n",
    "        return df.to_dict(orient='records')\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported data type: {type(data)}\")\n",
    "\n",
    "\n",
    "def score_model(dataset):\n",
    "    \"\"\"\n",
    "    Score data using the MLflow model serving endpoint.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Can be a DataFrame, numpy array, or list of lists\n",
    "    \n",
    "    Returns:\n",
    "        Predictions from the model\n",
    "    \"\"\"\n",
    "    url = 'https://adb-jdskd.15.azuredatabricks.net/serving-endpoints/revenue_forecasting_model_v2/invocations'\n",
    "    headers = {'Authorization': f'Bearer dkshdhska', 'Content-Type': 'application/json'}\n",
    "\n",
    "    # Convert to dataframe_records format with column names\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        records = dataset[FEATURE_NAMES].to_dict(orient='records')\n",
    "    else:\n",
    "        records = create_dataframe_records(dataset)\n",
    "    \n",
    "    # Use dataframe_records format - this includes column names!\n",
    "    ds_dict = {'dataframe_records': records}\n",
    "\n",
    "    data_json = json.dumps(ds_dict, allow_nan=True)\n",
    "\n",
    "    response = requests.request(method='POST', headers=headers, url=url, data=data_json)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Request failed with status {response.status_code}, {response.text}')\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Test data as list of lists (values in same order as FEATURE_NAMES)\n",
    "    test_values = [\n",
    "\n",
    "        [2140805032.76,\t12,\t508515179.11,\t-0.5,\t1,\t127898772.13,\t-94471806.81,\t222370578.94,\t-158596557.71,\t71536376.66416667,\t-0.0747306702941992,\t0,\t-1,\t158245828.04,\t0]\n",
    "    ]\n",
    "\n",
    "    # Convert to DataFrame with proper column names\n",
    "    test_df = pd.DataFrame(test_values, columns=FEATURE_NAMES)\n",
    "    \n",
    "    # Create payload with dataframe_records format (includes column names!)\n",
    "    test_data = {\n",
    "        \"dataframe_records\": test_df.to_dict(orient='records')\n",
    "    }\n",
    "\n",
    "    # Send the test data to the endpoint\n",
    "    url = 'https://adb-dshkds.15.azuredatabricks.net/serving-endpoints/revenue_forecasting_model_v2/invocations'\n",
    "\n",
    "    headers = {'Authorization': f'Bearer dkshds', 'Content-Type': 'application/json'}\n",
    "\n",
    "    data_json = json.dumps(test_data, allow_nan=True)\n",
    "\n",
    "    data_json = json.dumps(test_data, allow_nan=True)\n",
    "\n",
    "    response = requests.request(method='POST', headers=headers, url=url, data=data_json)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f'Request failed with status {response.status_code}, {response.text}')\n",
    "    else:\n",
    "        predictions = response.json()\n",
    "        print(\"Predictions:\")\n",
    "        print(json.dumps(predictions, indent=2))\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d6d96c4-46bc-4134-aaf6-bd9031a42cc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "\n",
    "model = mlflow.sklearn.load_model(\"models:/fpnacopilot.ml_model.revenue_forecasting_model_v2/1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be96f19f-58a8-4e02-aa6b-32393d38d35a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d29872-a585-420d-ab84-93979560b6e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(model.coef_)\n",
    "print(type(model).__name__)\n",
    "print(train_df)\n",
    "print(getattr(model, 'feature_names_in_', 'Attribute not available'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1668c77b-a224-46ed-bc46-85d2735ba201",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "[-56467.88336473   7575.74195926 -45804.68240135  60959.54863936\n",
    " -14946.88452612   7270.59006503  17913.58622299 -17655.75863062\n",
    "  -6440.9282813   -8101.91594051  -2482.72487924   6990.42243696\n",
    "  12716.8593275  -46442.46818755  23188.67997799]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2d6bcd7-a2da-4926-b23d-53d3b438bc23",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "load data and model"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "# import logging\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Configure logging\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO,\n",
    "#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "# )\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class InterpretabilityConfig:\n",
    "    \"\"\"Configuration for SHAP and LIME analysis.\"\"\"\n",
    "    shap_max_samples: int = 100  # Max samples for SHAP background\n",
    "    shap_explainer_type: str = \"auto\"  # auto, tree, kernel, linear\n",
    "    lime_num_features: int = 10\n",
    "    lime_num_samples: int = 5000\n",
    "    top_n_features: int = 10  # Top N features to include in summary\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Model Loading\n",
    "# =============================================================================\n",
    "\n",
    "def load_model_and_data():\n",
    "    \"\"\"\n",
    "    PLACEHOLDER: Replace this function with your actual model and data loading code.\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained ML model object\n",
    "        model_type: String name of model type (e.g., \"RandomForestRegressor\")\n",
    "        feature_names: List of feature names\n",
    "        preprocessor: Optional preprocessor/transformer pipeline\n",
    "        feature_selector: Optional feature selector\n",
    "        X_train: Training features (numpy array or DataFrame)\n",
    "        X_explain: Features to explain (numpy array or DataFrame)\n",
    "        y_train: Training target (optional)\n",
    "        y_explain: Target to explain (optional)\n",
    "        df_data: Full DataFrame with data for statistics\n",
    "        target_col: Name of target column\n",
    "        model_metrics: Dict with 'rmse', 'r2', 'mape' keys\n",
    "        months_lookback: Integer, number of months analyzed\n",
    "    \"\"\"\n",
    "    # TODO: Replace this placeholder with your actual code\n",
    "    # Example structure:\n",
    "    model_type = type(model).__name__\n",
    "    feature_names = config.FINAL_FEATURES\n",
    "    X_train = spark.read.table(\"fpnacopilot.machine_learning.trainfeaturedata\").toPandas().fillna(0)\n",
    "    X_explain = spark.read.table(\"fpnacopilot.machine_learning.testfeaturedata\").toPandas().iloc[:1].fillna(0)  # Data to explain\n",
    "\n",
    "    df_data = spark.read.table(\"fpnacopilot.machine_learning.fullfeaturedata\").toPandas().fillna(0) # Full DataFrame\n",
    "    model_metrics = result['metrics']\n",
    "    target_col = config.TARGET\n",
    "    preprocessor = None\n",
    "    feature_selector = None\n",
    "    y_train = None\n",
    "    y_explain = None\n",
    "    months_lookback = None\n",
    "\n",
    "    return  model, model_type, feature_names, preprocessor, feature_selector,X_train, X_explain, y_train, y_explain,df_data, target_col, model_metrics, months_lookback\n",
    "\n",
    "    \n",
    "    raise NotImplementedError(\n",
    "        \"Please replace this function with your actual model and data loading code.\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04f39ab9-8286-45af-b948-1afe95cc9423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "load_model_and_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e71a5ea-7fd1-4aac-8f07-a2ef75190d1b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Featrure Importance"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Feature Importance Extraction\n",
    "# =============================================================================\n",
    "\n",
    "class FeatureImportanceExtractor:\n",
    "    \"\"\"Extracts feature importance from trained models (from reference file).\"\"\"\n",
    "    \n",
    "    def __init__(self, model: Any, feature_names: List[str]):\n",
    "        \"\"\"\n",
    "        Initialize with model and feature names.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained ML model\n",
    "            feature_names: List of feature names\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.feature_names = feature_names\n",
    "    \n",
    "    def get_feature_importance(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract feature importance from the model.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with feature names and importance scores\n",
    "        \"\"\"\n",
    "        importance_values = self._extract_importance()\n",
    "        \n",
    "        if importance_values is None:\n",
    "            logger.warning(\"Could not extract feature importance from model\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'feature': self.feature_names[:len(importance_values)],\n",
    "            'importance': importance_values\n",
    "        })\n",
    "        \n",
    "        # Sort by importance and add rank\n",
    "        df = df.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "        df['rank'] = range(1, len(df) + 1)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _extract_importance(self) -> Optional[np.ndarray]:\n",
    "        \"\"\"Extract importance values based on model type.\"\"\"\n",
    "        model = self.model\n",
    "        \n",
    "        # Tree-based models (RF, GBR, XGBoost, LightGBM, CatBoost)\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            return model.feature_importances_\n",
    "        \n",
    "        # Linear models (coefficients)\n",
    "        if hasattr(model, 'coef_'):\n",
    "            return np.abs(model.coef_).flatten()\n",
    "        \n",
    "        # Try to get from nested estimator\n",
    "        if hasattr(model, 'estimator_') and hasattr(model.estimator_, 'feature_importances_'):\n",
    "            return model.estimator_.feature_importances_\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_top_features(self, n: int = 10) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Get top N most important features.\"\"\"\n",
    "        importance_df = self.get_feature_importance()\n",
    "        \n",
    "        if importance_df.empty:\n",
    "            return []\n",
    "        \n",
    "        top = importance_df.head(n)\n",
    "        return list(zip(top['feature'], top['importance']))\n",
    "    \n",
    "    def importance_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate a summary of feature importance.\"\"\"\n",
    "        importance_df = self.get_feature_importance()\n",
    "        \n",
    "        if importance_df.empty:\n",
    "            return {\"error\": \"Could not extract feature importance\"}\n",
    "        \n",
    "        return {\n",
    "            \"total_features\": len(importance_df),\n",
    "            \"top_10_features\": self.get_top_features(10),\n",
    "            \"importance_concentration\": {\n",
    "                \"top_5_pct\": importance_df.head(5)['importance'].sum() / importance_df['importance'].sum() * 100,\n",
    "                \"top_10_pct\": importance_df.head(10)['importance'].sum() / importance_df['importance'].sum() * 100,\n",
    "            },\n",
    "            \"full_importance_table\": importance_df.to_dict('records')\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "638b7e4a-2aa5-4cd2-8bba-0b4f2b0791e7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SHAP"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# SHAP Analysis Module\n",
    "# =============================================================================\n",
    "\n",
    "class SHAPAnalyzer:\n",
    "    \"\"\"Performs SHAP (SHapley Additive exPlanations) analysis.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model: Any, \n",
    "        feature_names: List[str],\n",
    "        config: InterpretabilityConfig\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.feature_names = feature_names\n",
    "        self.config = config\n",
    "        self._explainer = None\n",
    "        self._shap_values = None\n",
    "        self._background_data = None\n",
    "    \n",
    "    def _create_explainer(self, X_background: np.ndarray):\n",
    "        \"\"\"Create appropriate SHAP explainer based on model type.\"\"\"\n",
    "        try:\n",
    "            import shap\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"shap package not installed. Install with: pip install shap\"\n",
    "            )\n",
    "        \n",
    "        model_name = type(self.model).__name__\n",
    "        explainer_type = self.config.shap_explainer_type\n",
    "        \n",
    "        # Sample background data if too large\n",
    "        if len(X_background) > self.config.shap_max_samples:\n",
    "            indices = np.random.choice(\n",
    "                len(X_background), \n",
    "                self.config.shap_max_samples, \n",
    "                replace=False\n",
    "            )\n",
    "            X_background = X_background[indices]\n",
    "        \n",
    "        self._background_data = X_background\n",
    "        \n",
    "        logger.info(f\"Creating SHAP explainer for {model_name}\")\n",
    "        \n",
    "        # Auto-select explainer type\n",
    "        if explainer_type == \"auto\":\n",
    "            # Tree-based models\n",
    "            if model_name in ['RandomForestRegressor', 'GradientBoostingRegressor', \n",
    "                             'XGBRegressor', 'LGBMRegressor', 'CatBoostRegressor',\n",
    "                             'ExtraTreesRegressor', 'DecisionTreeRegressor']:\n",
    "                explainer_type = \"tree\"\n",
    "            # Linear models\n",
    "            elif model_name in ['LinearRegression', 'Ridge', 'Lasso', \n",
    "                               'ElasticNet', 'BayesianRidge', 'HuberRegressor']:\n",
    "                explainer_type = \"linear\"\n",
    "            else:\n",
    "                explainer_type = \"kernel\"\n",
    "        \n",
    "        # Create explainer\n",
    "        if explainer_type == \"tree\":\n",
    "            self._explainer = shap.TreeExplainer(self.model)\n",
    "        elif explainer_type == \"linear\":\n",
    "            self._explainer = shap.LinearExplainer(self.model, X_background)\n",
    "        else:\n",
    "            # Kernel explainer as fallback (slower but universal)\n",
    "            self._explainer = shap.KernelExplainer(\n",
    "                self.model.predict, \n",
    "                shap.sample(X_background, min(100, len(X_background)))\n",
    "            )\n",
    "        \n",
    "        return self._explainer\n",
    "    \n",
    "    def compute_shap_values(\n",
    "        self, \n",
    "        X_train: np.ndarray, \n",
    "        X_explain: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute SHAP values for explanation data.\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training data for background\n",
    "            X_explain: Data to explain\n",
    "            \n",
    "        Returns:\n",
    "            SHAP values array\n",
    "        \"\"\"\n",
    "        if self._explainer is None:\n",
    "            self._create_explainer(X_train)\n",
    "        \n",
    "        logger.info(f\"Computing SHAP values for {len(X_explain)} samples\")\n",
    "        \n",
    "        # Compute SHAP values\n",
    "        self._shap_values = self._explainer.shap_values(X_explain)\n",
    "        \n",
    "        # Handle different SHAP value formats\n",
    "        if isinstance(self._shap_values, list):\n",
    "            self._shap_values = self._shap_values[0]\n",
    "        \n",
    "        return self._shap_values\n",
    "    \n",
    "    def get_global_importance(self) -> pd.DataFrame:\n",
    "        \"\"\"Get global feature importance from SHAP values.\"\"\"\n",
    "        if self._shap_values is None:\n",
    "            raise ValueError(\"SHAP values not computed. Call compute_shap_values first.\")\n",
    "        \n",
    "        # Mean absolute SHAP values\n",
    "        mean_abs_shap = np.abs(self._shap_values).mean(axis=0)\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'feature': self.feature_names[:len(mean_abs_shap)],\n",
    "            'shap_importance': mean_abs_shap\n",
    "        })\n",
    "        \n",
    "        return df.sort_values('shap_importance', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    def generate_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive SHAP analysis summary.\"\"\"\n",
    "        global_importance = self.get_global_importance()\n",
    "        \n",
    "        return {\n",
    "            \"method\": \"SHAP\",\n",
    "            \"samples_analyzed\": len(self._shap_values) if self._shap_values is not None else 0,\n",
    "            \"top_features\": global_importance.head(self.config.top_n_features).to_dict('records'),\n",
    "            \"feature_importance_distribution\": {\n",
    "                \"mean\": float(global_importance['shap_importance'].mean()),\n",
    "                \"std\": float(global_importance['shap_importance'].std()),\n",
    "                \"max\": float(global_importance['shap_importance'].max()),\n",
    "                \"min\": float(global_importance['shap_importance'].min()),\n",
    "            },\n",
    "            \"interpretation\": self._generate_interpretation(global_importance)\n",
    "        }\n",
    "    \n",
    "    def _generate_interpretation(self, importance_df: pd.DataFrame) -> str:\n",
    "        \"\"\"Generate human-readable interpretation.\"\"\"\n",
    "        top = importance_df.head(3)\n",
    "        features = top['feature'].tolist()\n",
    "        \n",
    "        interpretation = f\"The top 3 most influential features are: {', '.join(features)}. \"\n",
    "        \n",
    "        # Check for feature concentration\n",
    "        top_5_pct = importance_df.head(5)['shap_importance'].sum() / importance_df['shap_importance'].sum() * 100\n",
    "        \n",
    "        if top_5_pct > 70:\n",
    "            interpretation += f\"These features show high concentration ({top_5_pct:.1f}% in top 5), suggesting the model relies heavily on a few key predictors.\"\n",
    "        else:\n",
    "            interpretation += f\"Feature importance is distributed across multiple features ({top_5_pct:.1f}% in top 5).\"\n",
    "        \n",
    "        return interpretation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000a8403-bc34-495c-b842-e2b45027295a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LIME"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# LIME Analysis Module\n",
    "# =============================================================================\n",
    "\n",
    "class LIMEAnalyzer:\n",
    "    \"\"\"Performs LIME (Local Interpretable Model-agnostic Explanations) analysis.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model: Any, \n",
    "        feature_names: List[str],\n",
    "        config: InterpretabilityConfig\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.feature_names = feature_names\n",
    "        self.config = config\n",
    "        self._explainer = None\n",
    "        self._explanations = []\n",
    "    \n",
    "    def _create_explainer(self, X_train: np.ndarray):\n",
    "        \"\"\"Create LIME explainer.\"\"\"\n",
    "        try:\n",
    "            from lime import lime_tabular\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"lime package not installed. Install with: pip install lime\"\n",
    "            )\n",
    "        \n",
    "        self._explainer = lime_tabular.LimeTabularExplainer(\n",
    "            training_data=X_train,\n",
    "            feature_names=self.feature_names,\n",
    "            mode='regression',\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        return self._explainer\n",
    "    \n",
    "    def explain_instances(\n",
    "        self, \n",
    "        X_train: np.ndarray,\n",
    "        X_explain: np.ndarray,\n",
    "        num_samples: Optional[int] = None\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Generate LIME explanations for multiple instances.\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training data\n",
    "            X_explain: Instances to explain\n",
    "            num_samples: Number of instances to explain (default: all)\n",
    "            \n",
    "        Returns:\n",
    "            List of explanation dictionaries\n",
    "        \"\"\"\n",
    "        if self._explainer is None:\n",
    "            self._create_explainer(X_train)\n",
    "        \n",
    "        if num_samples is None or num_samples > len(X_explain):\n",
    "            num_samples = min(len(X_explain), 50)  # Limit for performance\n",
    "        \n",
    "        logger.info(f\"Generating LIME explanations for {num_samples} instances\")\n",
    "        \n",
    "        self._explanations = []\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            try:\n",
    "                exp = self._explainer.explain_instance(\n",
    "                    X_explain[i],\n",
    "                    self.model.predict,\n",
    "                    num_features=self.config.lime_num_features,\n",
    "                    num_samples=self.config.lime_num_samples\n",
    "                )\n",
    "                \n",
    "                self._explanations.append({\n",
    "                    'instance_idx': i,\n",
    "                    'prediction': self.model.predict(X_explain[i:i+1])[0],\n",
    "                    'local_explanation': dict(exp.as_list()),\n",
    "                    'intercept': exp.intercept[0] if hasattr(exp, 'intercept') else None,\n",
    "                    'score': exp.score if hasattr(exp, 'score') else None\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to explain instance {i}: {e}\")\n",
    "        \n",
    "        return self._explanations\n",
    "    \n",
    "    def aggregate_explanations(self) -> pd.DataFrame:\n",
    "        \"\"\"Aggregate local explanations to get global feature importance.\"\"\"\n",
    "        if not self._explanations:\n",
    "            raise ValueError(\"No explanations computed. Call explain_instances first.\")\n",
    "        \n",
    "        # Collect all feature contributions\n",
    "        all_contributions = {}\n",
    "        \n",
    "        for exp in self._explanations:\n",
    "            for feature, contribution in exp['local_explanation'].items():\n",
    "                if feature not in all_contributions:\n",
    "                    all_contributions[feature] = []\n",
    "                all_contributions[feature].append(abs(contribution))\n",
    "        \n",
    "        # Calculate mean absolute contribution\n",
    "        aggregated = {\n",
    "            'feature': list(all_contributions.keys()),\n",
    "            'lime_importance': [np.mean(v) for v in all_contributions.values()],\n",
    "            'lime_std': [np.std(v) for v in all_contributions.values()]\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(aggregated)\n",
    "        return df.sort_values('lime_importance', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    def generate_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive LIME analysis summary.\"\"\"\n",
    "        aggregated = self.aggregate_explanations()\n",
    "        \n",
    "        # Identify consistent vs variable features\n",
    "        consistent_features = aggregated[\n",
    "            aggregated['lime_std'] / aggregated['lime_importance'] < 0.5\n",
    "        ]['feature'].tolist()[:5]\n",
    "        \n",
    "        variable_features = aggregated[\n",
    "            aggregated['lime_std'] / aggregated['lime_importance'] >= 0.5\n",
    "        ]['feature'].tolist()[:5]\n",
    "        \n",
    "        return {\n",
    "            \"method\": \"LIME\",\n",
    "            \"instances_explained\": len(self._explanations),\n",
    "            \"top_features\": aggregated.head(self.config.top_n_features).to_dict('records'),\n",
    "            \"consistent_features\": consistent_features,\n",
    "            \"variable_features\": variable_features,\n",
    "            \"model_fidelity\": {\n",
    "                \"mean_score\": np.mean([e.get('score', 0) for e in self._explanations if e.get('score')]),\n",
    "            },\n",
    "            \"interpretation\": self._generate_interpretation(aggregated)\n",
    "        }\n",
    "    \n",
    "    def _generate_interpretation(self, aggregated: pd.DataFrame) -> str:\n",
    "        \"\"\"Generate human-readable interpretation.\"\"\"\n",
    "        top = aggregated.head(3)\n",
    "        \n",
    "        interpretation = f\"LIME analysis reveals that {top['feature'].iloc[0]} has the highest local importance \"\n",
    "        interpretation += f\"(avg contribution: {top['lime_importance'].iloc[0]:.4f}). \"\n",
    "        \n",
    "        # Check consistency\n",
    "        if len(aggregated) > 0:\n",
    "            high_variability = aggregated[\n",
    "                aggregated['lime_std'] > aggregated['lime_importance'] * 0.5\n",
    "            ]\n",
    "            if len(high_variability) > 0:\n",
    "                interpretation += f\"Features with high variability across instances: {', '.join(high_variability['feature'].head(3).tolist())}.\"\n",
    "        \n",
    "        return interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7c345e-bc88-40ac-afcb-1e44fafbde92",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "data stats"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Data Statistics Helper\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_data_patterns(df: pd.DataFrame, target_col: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze patterns in the data (matching reference implementation).\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with data\n",
    "        target_col: Name of target column\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with data statistics\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return {}\n",
    "    \n",
    "    stats = {\n",
    "        \"total_records\": len(df),\n",
    "        \"num_features\": len(df.columns),\n",
    "        \"missing_values\": int(df.isnull().sum().sum())\n",
    "    }\n",
    "    \n",
    "    if target_col in df.columns:\n",
    "        target = df[target_col]\n",
    "        stats[\"target_stats\"] = {\n",
    "            \"mean\": float(target.mean()),\n",
    "            \"std\": float(target.std()),\n",
    "            \"min\": float(target.min()),\n",
    "            \"max\": float(target.max()),\n",
    "            \"trend\": \"Increasing\" if target.tail(10).mean() > target.mean() else \"Decreasing\"\n",
    "        }\n",
    "    \n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b48a1060-074e-4cf2-aa4b-b0a5d5ffb1ed",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "blob storage upload"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Blob Storage Helper (Databricks)\n",
    "# =============================================================================\n",
    "\n",
    "def save_to_blob_storage(\n",
    "    container_name: str,\n",
    "    blob_name: str,\n",
    "    data: Dict[str, Any],\n",
    "    storage_account: str = None,\n",
    "    account_key: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Save JSON data to Azure Blob Storage using BlobServiceClient.\n",
    "    \n",
    "    Args:\n",
    "        container_name: Name of the blob container\n",
    "        blob_name: Name of the blob file\n",
    "        data: Dictionary to save as JSON\n",
    "        storage_account: Azure Storage account name (required)\n",
    "        account_key: Azure Storage account key (required)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert to JSON string\n",
    "        json_str = json.dumps(data, indent=2, default=str)\n",
    "        \n",
    "        # Validate required parameters\n",
    "        if not storage_account or not account_key:\n",
    "            raise ValueError(\"storage_account and account_key are required\")\n",
    "        \n",
    "        # Use Azure Storage SDK with BlobServiceClient\n",
    "        from azure.storage.blob import BlobServiceClient, ContentSettings\n",
    "        \n",
    "        connection_string = (\n",
    "            f\"DefaultEndpointsProtocol=https;\"\n",
    "            f\"AccountName={storage_account};\"\n",
    "            f\"AccountKey={account_key};\"\n",
    "            f\"EndpointSuffix=core.windows.net\"\n",
    "        )\n",
    "        \n",
    "        blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "        container_client = blob_service_client.get_container_client(container_name)\n",
    "        blob_client = container_client.get_blob_client(blob_name)\n",
    "        \n",
    "        blob_client.upload_blob(\n",
    "            json_str,\n",
    "            overwrite=True,\n",
    "            content_settings=ContentSettings(content_type=\"application/json\")\n",
    "        )\n",
    "        logger.info(f\"Saved to blob storage: {container_name}/{blob_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving to blob storage: {e}\", exc_info=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a42105d1-8922-4a86-8475-9e5d888dd161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Main Pipeline\n",
    "# =============================================================================\n",
    "\n",
    "def run_shap_lime_analysis(\n",
    "    year: int,\n",
    "    month: int,\n",
    "    shap_container: str = \"shap-results\",\n",
    "    lime_container: str = \"lime-results\",\n",
    "    storage_account: str = None,\n",
    "    account_key: str = None,\n",
    "    config: InterpretabilityConfig = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to run SHAP and LIME analysis and save results to blob storage.\n",
    "    \n",
    "    Args:\n",
    "        year: Year for analysis (e.g., 2024)\n",
    "        month: Month for analysis (1-12)\n",
    "        shap_container: Azure Blob Storage container for SHAP results\n",
    "        lime_container: Azure Blob Storage container for LIME results\n",
    "        storage_account: Azure Storage account name\n",
    "        account_key: Azure Storage account key\n",
    "        config: InterpretabilityConfig object (uses defaults if None)\n",
    "    \"\"\"\n",
    "    if not (1 <= month <= 12):\n",
    "        raise ValueError(f\"Invalid month: {month}. Must be between 1 and 12\")\n",
    "    \n",
    "    if config is None:\n",
    "        config = InterpretabilityConfig()\n",
    "    \n",
    "    logger.info(f\"Starting SHAP and LIME analysis for {year}-{month:02d}\")\n",
    "    \n",
    "    # Step 1: Load model and data (PLACEHOLDER - replace with your code)\n",
    "    logger.info(\"Loading model and data...\")\n",
    "    (\n",
    "        model, model_type, feature_names, preprocessor, feature_selector,\n",
    "        X_train, X_explain, y_train, y_explain,\n",
    "        df_data, target_col, model_metrics, months_lookback\n",
    "    ) = load_model_and_data()\n",
    "\n",
    "\n",
    "    \n",
    "    # Ensure X_train and X_explain are numpy arrays\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        X_train = X_train.values\n",
    "    if isinstance(X_explain, pd.DataFrame):\n",
    "        X_explain = X_explain.values\n",
    "    \n",
    "    # Apply preprocessing if needed\n",
    "    if preprocessor is not None:\n",
    "        X_train = preprocessor.transform(X_train) if hasattr(preprocessor, 'transform') else X_train\n",
    "        X_explain = preprocessor.transform(X_explain) if hasattr(preprocessor, 'transform') else X_explain\n",
    "    \n",
    "    # Apply feature selection if needed\n",
    "    if feature_selector is not None:\n",
    "        X_train = feature_selector.transform(X_train) if hasattr(feature_selector, 'transform') else X_train\n",
    "        X_explain = feature_selector.transform(X_explain) if hasattr(feature_selector, 'transform') else X_explain\n",
    "    \n",
    "    # Step 2: Extract feature importance\n",
    "    logger.info(\"Extracting feature importance from model...\")\n",
    "    feature_extractor = FeatureImportanceExtractor(model, feature_names)\n",
    "    feature_importance_summary = feature_extractor.importance_summary()\n",
    "    \n",
    "    # Step 3: Analyze data patterns\n",
    "    logger.info(\"Analyzing data patterns...\")\n",
    "    data_stats = analyze_data_patterns(df_data, target_col)\n",
    "    \n",
    "    # Step 4: Run SHAP analysis\n",
    "    logger.info(\"Running SHAP analysis...\")\n",
    "    shap_analyzer = SHAPAnalyzer(model, feature_names, config)\n",
    "    shap_analyzer.compute_shap_values(X_train, X_explain)\n",
    "    shap_summary = shap_analyzer.generate_summary()\n",
    "    \n",
    "    # Step 5: Run LIME analysis\n",
    "    logger.info(\"Running LIME analysis...\")\n",
    "    lime_analyzer = LIMEAnalyzer(model, feature_names, config)\n",
    "    lime_analyzer.explain_instances(X_train, X_explain)\n",
    "    lime_summary = lime_analyzer.generate_summary()\n",
    "    \n",
    "    # Step 6: Prepare metadata\n",
    "    metadata = {\n",
    "        \"model_type\": model_type,\n",
    "        \"target_variable\": target_col,\n",
    "        \"months_lookback\": months_lookback,\n",
    "        \"total_records\": len(df_data) if df_data is not None else 0,\n",
    "        \"num_features\": len(feature_names),\n",
    "        \"feature_importance\": {\n",
    "            \"top_10_features\": feature_importance_summary.get(\"top_10_features\", [])\n",
    "        },\n",
    "        \"data_stats\": data_stats\n",
    "    }\n",
    "    \n",
    "    # Step 7: Build complete SHAP result with metadata\n",
    "    shap_result = {\n",
    "        **shap_summary,\n",
    "        \"metadata\": metadata,\n",
    "        \"model_metrics\": model_metrics,\n",
    "        \"feature_importance\": {\n",
    "            \"top_10_features\": feature_importance_summary.get(\"top_10_features\", [])\n",
    "        },\n",
    "        \"data_stats\": data_stats\n",
    "    }\n",
    "    \n",
    "    # Step 8: Build complete LIME result with metadata\n",
    "    lime_result = {\n",
    "        **lime_summary,\n",
    "        \"metadata\": metadata,\n",
    "        \"model_metrics\": model_metrics,\n",
    "        \"feature_importance\": {\n",
    "            \"top_10_features\": feature_importance_summary.get(\"top_10_features\", [])\n",
    "        },\n",
    "        \"data_stats\": data_stats\n",
    "    }\n",
    "    \n",
    "    # Step 9: Save to blob storage\n",
    "    shap_blob_name = f\"shap_{year}_{month:02d}.json\"\n",
    "    lime_blob_name = f\"lime_{year}_{month:02d}.json\"\n",
    "    \n",
    "    logger.info(f\"Saving SHAP results to {shap_container}/{shap_blob_name}...\")\n",
    "    save_to_blob_storage(\n",
    "        shap_container,\n",
    "        shap_blob_name,\n",
    "        shap_result,\n",
    "        storage_account,\n",
    "        account_key\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Saving LIME results to {lime_container}/{lime_blob_name}...\")\n",
    "    save_to_blob_storage(\n",
    "        lime_container,\n",
    "        lime_blob_name,\n",
    "        lime_result,\n",
    "        storage_account,\n",
    "        account_key\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Successfully completed SHAP and LIME analysis for {year}-{month:02d}\")\n",
    "    logger.info(f\"SHAP results saved to: {shap_container}/{shap_blob_name}\")\n",
    "    logger.info(f\"LIME results saved to: {lime_container}/{lime_blob_name}\")\n",
    "    \n",
    "    return shap_result, lime_result\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Example Usage\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example configuration\n",
    "    YEAR = 2024\n",
    "    MONTH = 1\n",
    "    SHAP_CONTAINER = \"shap-results\"\n",
    "    LIME_CONTAINER = \"lime-results\"\n",
    "    STORAGE_ACCOUNT = \"gtairnistorage\"  # Replace with actual storage account\n",
    "    ACCOUNT_KEY = access_key  # Optional, can use dbutils secrets instead\n",
    "    \n",
    "    # Run analysis\n",
    "    try:\n",
    "        shap_result, lime_result = run_shap_lime_analysis(\n",
    "            year=YEAR,\n",
    "            month=MONTH,\n",
    "            shap_container=SHAP_CONTAINER,\n",
    "            lime_container=LIME_CONTAINER,\n",
    "            storage_account=STORAGE_ACCOUNT,\n",
    "            account_key=ACCOUNT_KEY\n",
    "        )\n",
    "        print(\"Analysis completed successfully!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        print(\"Please implement the load_model_and_data() function with your model loading code.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        raise\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Model pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
